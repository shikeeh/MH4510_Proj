final_results <- data.frame(Model = c('MLR', 'Random Forest', 'XGBoost', 'MARS', 'PLS'),
RMSE = c(16.20561, 21.6223370884116, 16.5914, 30.0025, 23.87552))
knitr::kable(final_results, "simple")
final_results <- data.frame(Model = c('MLR', 'Random Forest', 'XGBoost', 'MARS', 'PLS'),
RMSE = c(16.20561, 21.6223370884116, 16.5914, 30.0025, 23.87552))
knitr::kable(final_results, "pipe")
knitr::kable(final_results, "simple")
lambda_for_plotting <- 10^seq(from = -1, to = 1.5, length = 100)
lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
as.matrix %>% t %>% as_tibble %>%
mutate(lambda = lambda_for_plotting)
lasso_coefs %>%
pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
geom_line() + scale_x_log10() + theme(legend.position = "none")
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", fill="", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words") +
scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words") +
scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", fill="blue", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words")
knitr::kable(head(D_dtm[, 1:5]), "simple")
D_dtm <- dtm %>%
as.matrix %>%
as_tibble %>%
mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_dtm[, 1:5]), "simple")
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(knitr)
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
knitr::kable(head(D[, 2:6]), "simple")
knitr::kable(head(D[, 6:10]), "simple")
head(D)
knitr::kable(head(D[, 21:25]), "simple")
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(knitr)
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D[, 21:25]), "simple")
D_clean <- D %>%
# create a new variable containing only lowercase text from Job.Description
mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
mutate(cleaned_text = tolower(cleaned_text))
head(D_clean)
knitr::kable(head(D[, c(21:24, 43)]), "simple")
knitr::kable(head(D[, c(21, 22, 23, 24, 43)]), "simple")
knitr::kable(head(D[, 39:43]), caption = "An example table caption.", "simple")
knitr::kable(head(D_clean[, c(21:24, 43)]), caption = "An example table caption.", "simple")
knitr::kable(head(D[, 20:25]), "simple")
knitr::kable(head(D_clean[, c(20:24, 43)]), caption = "An example table caption.", "simple")
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D[, 20:25]),
caption = "A sample of our dataset.",
"simple")
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
# create a new variable containing only lowercase text from Job.Description
mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
mutate(cleaned_text = tolower(cleaned_text))
head(D_clean)
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D_clean[, c(20:24, 43)]),
caption = "Another sample of our dataset.",
"simple")
#remove unnecessary columns
D_clean <- D_clean %>%
select(c(Avg.Salary.K., cleaned_text))
knitr::kable(head(D_clean),
caption = "Another sample of our dataset.",
"simple")
ggplot(D_clean, aes(x=Avg.Salary.K.)) +
geom_histogram(fill = "blue", alpha = 0.65, bins = 20)
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
geom_boxplot(fill = "blue", alpha = 0.65)
visualize_text <- function(x) {
# x is a character vector
# the function will extract
frequent_words <- termFreq(x)
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words),
freq = frequent_words, min.freq = 40,
max.words = 100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "Dark2"))
}
visualize_text(D_clean$cleaned_text)
visualize_text <- function(x) {
# x is a character vector
# the function will extract
frequent_words <- termFreq(x)
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words),
freq = frequent_words, min.freq = 40,
max.words = 100, random.order=FALSE, rot.per=0.35
}
visualize_text <- function(x) {
# x is a character vector
# the function will extract
frequent_words <- termFreq(x)
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words),
freq = frequent_words, min.freq = 40,
max.words = 100, random.order=FALSE, rot.per=0.35)
}
visualize_text(D_clean$cleaned_text)
visualize_text <- function(x) {
# x is a character vector
# the function will extract
frequent_words <- termFreq(x)
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words),
freq = frequent_words, min.freq = 40,
max.words = 100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "Dark2"))
}
visualize_text(D_clean$cleaned_text)
visualize_text <- function(x) {
# x is a character vector
# the function will extract
frequent_words <- termFreq(x)
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words),
freq = frequent_words, min.freq = 40,
max.words = 100, random.order=FALSE, rot.per=0.35,
colors=brewer.pal(9, "Dark2"))
}
visualize_text(D_clean$cleaned_text)
corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)
words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]
cat("Setting minimum word frequency to 40, we retain", length(frequent_words), "words out of the original", length(words_freq), "words.", "\n")
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
dtm <- dtm[ , names(frequent_words)]
cat("Our document-term matrix consists of", nrow(dtm), "job descriptions against", ncol(dtm), 'words present in our vocab after removing stopwords.', '\n')
D_dtm <- dtm %>%
as.matrix %>%
as_tibble %>%
mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_dtm[, 1:5]), "simple")
D_dtm <- dtm %>%
as.matrix %>%
as_tibble %>%
mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_dtm[, 1:5]),
caption = "A sample of our DTM."
"simple")
D_dtm <- dtm %>%
as.matrix %>%
as_tibble %>%
mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_dtm[, 1:5]),
caption = "A sample of our DTM.",
"simple")
lambda <- 10^seq(-1, 0 , length = 10)
lambda
set.seed(100)
lasso <- train(
Y_salary ~., data = D_dtm, method = "glmnet",
trControl = trainControl("cv", number = 10),
tuneGrid = expand.grid(alpha = 1, lambda = lambda),
preProcess = c("center","scale")
)
lasso
lambda_for_plotting <- 10^seq(from = -1, to = 1.5, length = 100)
lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
as.matrix %>% t %>% as_tibble %>%
mutate(lambda = lambda_for_plotting)
lasso_coefs %>%
pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
geom_line() + scale_x_log10() + theme(legend.position = "none")
store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>%
select_if(function(col) !all(col == 0))
selected_var <- names(store[2:(ncol(store)-1)])
head(selected_var)
store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>%
select_if(function(col) !all(col == 0))
selected_var <- names(store[2:(ncol(store)-1)])
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
mutate(Y_salary = D_dtm$Y_salary) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_final[, 1:6]),
caption = "A sample of our final dataset.",
"simple")
head(D_final)
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
mutate(Y_salary = D_dtm$Y_salary) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_final[, 1:6]),
caption = "A sample of our final dataset.",
"simple")
set.seed(100)
ind <- runif(nrow(D_final)) < 0.8
train_data <- D_final[ind , ]
test_data <- D_final[!ind , ]
cat("Dimensions of the training set are", dim(train_data), "\n")
cat("Dimensions of the test set are", dim(test_data), "\n")
mlr_mod <- lm(Y_salary ~ .,
data = train_data)
mlr_mod %>%
predict(test_data) %>%
rmse(test_data$Y_salary)
var_coeff <- data.frame(coefficients(mlr_mod)) %>%
arrange(desc(abs(coefficients(mlr_mod)))) %>%
dplyr::slice(2:21)
ggplot(var_coeff, aes(x=abs(coefficients.mlr_mod.),
y=reorder(rownames(var_coeff), +abs(coefficients.mlr_mod.)))) +
geom_bar(stat = "identity", fill="blue", alpha = 0.65) +
xlab("Absolute Coefficients") + ylab("Key words") +
coord_cartesian(xlim = c(10, 25))
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", fill="blue", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words")
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", fill="green", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words")
ggplot(var_coeff, aes(x=coefficients.mlr_mod.,
y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
xlab("Coefficients") + ylab("Key words")
var_coeff <- data.frame(coefficients(mlr_mod)) %>%
arrange(desc(abs(coefficients(mlr_mod)))) %>%
dplyr::slice(2:21)
ggplot(var_coeff, aes(x=abs(coefficients.mlr_mod.),
y=reorder(rownames(var_coeff), +abs(coefficients.mlr_mod.)))) +
geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
xlab("Absolute Coefficients") + ylab("Key words") +
coord_cartesian(xlim = c(10, 25))
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
geom_boxplot(fill = "darkgreen", alpha = 0.65)
ggplot(D_clean, aes(x=Avg.Salary.K.)) +
geom_histogram(fill = "darkgreen", alpha = 0.65, bins = 20)
knitr::kable(mod_rf$bestTune)
mod_rf$bestTune
knitr::kable(mod_rf$bestTune)
knitr::kable(mod_rf$bestTune, "simple")
knitr::kable(mod_rf$bestTune)
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D[, 20:25]),
caption = "A sample of our dataset.")
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
# create a new variable containing only lowercase text from Job.Description
mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
mutate(cleaned_text = tolower(cleaned_text))
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D_clean[, c(20:24, 43)]),
caption = "Another sample of our dataset.")
#remove unnecessary columns
D_clean <- D_clean %>%
select(c(Avg.Salary.K., cleaned_text))
knitr::kable(head(D_clean),
caption = "Our key variables.")
D_dtm <- dtm %>%
as.matrix %>%
as_tibble %>%
mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_dtm[, 1:5]),
caption = "A sample of our DTM.")
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
mutate(Y_salary = D_dtm$Y_salary) %>%
select(c(Y_salary, everything()))
knitr::kable(head(D_final[, 1:6]),
caption = "A sample of our final dataset.")
rfGrid <- expand.grid(mtry = 178,
min.node.size = 5,
splitrule = "variance")
mod_rf_tune <- train(Y_salary ~., data = D_final, method = "ranger",
num.trees = 50, importance = 'impurity',
tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
knitr::kable(mod_rf$bestTune,
caption = "The optimal values of the hyper-parameters are:")
knitr::kable(mod_rf$bestTune)
#random forest regression
train_data_rf <- train_data %>%
select(-c(Y_salary))
test_data_rf <- test_data %>%
select(-c(Y_salary))
regr <- randomForest(x = train_data_rf,
y = train_data$Y_salary,
maxnodes = 100 ,
ntree = 1000)
predictions <- predict(regr, test_data_rf)
result <- test_data
result['predictions'] <- predictions
knitr::kable(head(result[, 1:6]),
caption = "A sample of our prediction results.")
#plot graph to compare actual vs. predicted
ggplot(  ) +
geom_point( aes(x = test_data$Y_salary, y = predictions, color = 'red', alpha = 0.5) ) +
geom_point( aes(x = test_data$Y_salary, y = predictions, color = 'blue',  alpha = 0.5)) +
labs(x = "Actual Average Salary", y = "Predicted Average Salary", color = "", alpha = 'Transperency') +
scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , test_data$Y_salary) ))
print(paste0('RMSE: ' ,caret::postResample(predictions , test_data$Y_salary)['RMSE'])) #21.44
#top 20 most important predictors
var_importance = mod_rf_tune$finalModel$variable.importance %>%
sort(decreasing = TRUE) %>% head(20)
var_importance
data.frame(variable = names(var_importance), importance = var_importance) %>%
ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() +
xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
#top 20 most important predictors
var_importance = mod_rf_tune$finalModel$variable.importance %>%
sort(decreasing = TRUE) %>% head(20)
data.frame(variable = names(var_importance), importance = var_importance) %>%
ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() +
xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
set.seed(100)
trControl <- trainControl(
method = 'cv',
number = 5,
verboseIter = TRUE,
allowParallel = TRUE)
xgbGrid <- expand.grid(
nrounds = 650,
eta = 0.21,
max_depth = 3,
gamma = 0.04,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
xgb_caret <- train(Y_salary ~.,
data = train_data,
method = "xgbTree",
trControl = trControl,
tuneGrid = xgbGrid)
xgb_caret
pred_y = predict(xgb_caret, test_data)
#measure prediction accuracy
caret::MAE(test_data$Y_salary, pred_y) #mae
caret::RMSE(test_data$Y_salary, pred_y) #rmse
knitr::kable(xgb_imp,
caption = "Our 20 most important features.")
knitr::kable(head(xgb_imp),
caption = "Some of our 20 most important features.")
xgb.plot.importance(xgb_imp[1:20])
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(knitr)
library(earth)
library(vip)
library(dplyr)
set.seed(100)
ind_mars <- which(runif(nrow(D_dtm)) < 0.7)
train_mars <- D_dtm %>% dplyr::slice(ind_mars)
test_mars <- D_dtm %>% dplyr::slice(-ind_mars)
cat("Dimensions of the training dataset are", dim(train_mars), "\n")
cat("Dimensions of the test dataset are", dim(test_mars), "\n")
knitr::kable(mars_cv$bestTune) #nprune = 11, degree = 1
knitr::kable(mars_cv$bestTune,
caption = "Our chosen parameters") #nprune = 11, degree = 1
mars_cv$results %>%
filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree)
knitr::kable(mars_cv$results %>%
filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree))
ggplot(mars_cv)
plot_GCV <- vip(mars_cv, num_features = 20, geom = "point", value = "gcv") + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "point", value = "rss") + ggtitle("RSS")
gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
set.seed(100)
# 80/20 split
inTraining <- createDataPartition(D_dtm$Y_salary, p = .7, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]
set.seed(100)
plsGrid <- expand.grid(
ncomp   = seq(1, 100, by = 1)
)
pls_cv <- train(
Y_salary ~ .,
data = train_pls,
method = 'pls',
metric = "RMSE",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneGrid = plsGrid
)
pls_cv
plot(pls_cv)
set.seed(100)
plsGrid <- expand.grid(
ncomp   = seq(1, 100, by = 1)
)
pls_cv <- train(
Y_salary ~ .,
data = train_pls,
method = 'pls',
metric = "RMSE",
preProcess = c("center", "scale"),
trControl = trainControl(method = "cv", number = 10),
tuneGrid = plsGrid
)
pls_cv
plot(pls_cv)
set.seed(100)
plsGrid_tuned <- expand.grid(
ncomp = 9
)
pls_cv_tuned <- train(Y_salary ~., data = train_pls,
method = "pls",
metric = "RMSE",
trControl = trainControl(method = "cv", number = 10),
tuneGrid = plsGrid_tuned
)
pred_pls = predict(pls_cv_tuned, test_pls)
print(caret::MAE(test_pls$Y_salary, pred_pls)) #16.63214
print(caret::RMSE(test_pls$Y_salary, pred_pls)) #23.87552
coefficients = coef(pls_cv_tuned$finalModel)
sum.coef = sum(sapply(coefficients, abs))
coefficients = coefficients * 100 / sum.coef
coefficients = sort(coefficients[, 1 , 1])
barplot(tail(coefficients, 5))
barplot(head(coefficients, 5))
coefficients = coef(pls_cv_tuned$finalModel)
sum.coef = sum(sapply(coefficients, abs))
coefficients = coefficients * 100 / sum.coef
coefficients = sort(coefficients[, 1 , 1])
barplot(tail(coefficients, 5))
barplot(head(coefficients, 5))
final_results <- data.frame(Model = c('MLR', 'Random Forest', 'XGBoost', 'MARS', 'PLS'),
RMSE = c(16.20561, 21.6223370884116, 16.5914, 30.0025, 23.87552))
knitr::kable(final_results,
caption = "Accuracy of models.")
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D[, 20:25]),
caption = "A sample of our dataset.")
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
# create a new variable containing only lowercase text from Job.Description
mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
mutate(cleaned_text = tolower(cleaned_text))
# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
knitr::kable(head(D_clean[, c(20:24, 43)]),
caption = "Another sample of our dataset.")
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
# create a new variable containing only lowercase text from Job.Description
mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
mutate(cleaned_text = tolower(cleaned_text))
#remove unnecessary columns
D_clean <- D_clean %>%
select(c(Avg.Salary.K., cleaned_text))
