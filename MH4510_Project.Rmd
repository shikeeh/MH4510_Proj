---
title: "**Studying the Impact of Job Descriptions on Data Science Salaries**"
subtitle: "Team Humilty"
date: "`r Sys.Date()`"
author: 
  - CHLOE NEO TZE CHING
  - ENG JING KEAT
  - GOH WEI LUN GLENN
  - KANEKO YOSHIKI
  - TAN WEI KEONG
output: 
  pdf_document:
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
urlcolor: blue
abstract: "Millions of workers worldwide are no longer satisfied with just a fair paycheck, instead, they wish to achieve job satisfaction through meaningful career opportunities. This project aims to study the impact of job descriptions on data science salaries. The data is cleaned by preparing job description text for analysis and converting it into a document-term matrix. The models used are Multiple Linear Regression (MLR), Random Forest (RF), XGBoost (XGB), Multivariate Adaptive Regression Splines (MARS), and Partial Least Squares (PLS), where their performances will be assessed in relation to one another. We found that MLR has the lowest Root Mean Square Error (RMSE) of 16.2. Thus MLR is the best performing model that accurately predicts salary, and we identified keywords used in this model."
editor_options: 
  markdown: 
    wrap: 72
---

\newpage

# Introduction to the problem

## Literature review

A rapid shift towards digitalisation of businesses has radically changed
the employment landscape in Singapore, which means Singaporeans need to
keep up with the changes if they wish to stay competitive at work (My
Skills Future, 2021). Employers in Singapore are starting to place an
emphasis on skills rather than education (Tan, 2021). New hires today
are assessed not just by their qualifications and work history, but also
by their soft skills as there are a variety of soft skills in demand
(The Straits Times, 2021).

According to a new report by Instant Offices, 73% of Singaporean workers
are dissatisfied with their jobs (Arora, 2022). When asked if they
planned to change jobs over the following six months, 31% of respondents
responded "yes" (Chong, 2022). Millions of workers worldwide are no
longer willing to return home with just a fair paycheck. They prefer to
know how well they are progressing towards a meaningful career, which is
wellness, freedom, security, and experience at work, so as to achieve
job satisfaction. As a result, job seekers should take all these factors
into consideration when applying for a job.

These factors can be broken down and identified as keywords in job
descriptions. Keywords are crucial to job adverts because they allow job
seekers to narrow their search related to a role, skill, or industry for
suitable employment (Alexander, 2019). Suitable individuals are more
likely to find the job post when employers and recruiters add key terms
and phrases that are relevant to a particular role. This increases the
percentage of successfully matching job seekers with their ideal jobs.

## Objective

This project aims to study the impact of job descriptions on data science 
salaries. Doing so will give insight to keywords in job descriptions that 
have a significant impact on salaries. Job seekers will also be able to set 
healthy salary expectations based on keywords that reflect their skill set 
and values. Identified keywords can also give insight to areas that job 
seekers can look to upskill in.

# Data set

## Description of data set

[Click
here](https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor)
to access our data set. The owner scrapped the data from Glassdoor and
pre-processed it. It contains 41 variables, including the average data
science salary measured in thousands, and has 742 observations.

```{r, include = FALSE}
# Import relevant libraries:
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(knitr)
library(earth)
library(vip)
library(dplyr)
library(pls)
```

Below is a sample of our dataset:

```{r, include = FALSE}
# import data from local csv file
D <- read_csv("data_cleaned_2021.csv")
```

```{r, echo = FALSE}
knitr::kable(head(D[, 20:25]), 
             caption = "A sample of our dataset.")
```

\newpage

## Exploratory data analysis

### Feature selection

Since this project focuses on job description and its relationship to
salary, we will only keep the "Job Description" variable and the "Avg
Salary(K)" variable. In the next section, we process our "Job
Description" variable so that it is usable for our models.

### Data cleaning

We clean our "Job Description" variable by creating a new variable
"cleaned_text" that stores only alphanumeric characters and converts all
the text to lowercase. We then only keep the "Avg Salary(K)" and
"cleaned_text" variables.

```{r, echo = FALSE}
# Keep only alphanumeric characters and spaces, change texts to lowercase, 
# and remove all "\n"
D_clean <- D %>%
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", D$`Job Description`)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) 

# remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(`Avg Salary(K)`, cleaned_text))
```

### Data visualizations

To get a better understanding of the data we are working with, we
perform some data visualizations. First, we take a look at the
distribution of the average salary (in thousands).

```{r, echo = FALSE, fig.cap="\\label{fig:figs}Histogram of salaries"}
ggplot(D_clean, aes(x=`Avg Salary(K)`)) + 
  geom_histogram(fill = "darkgreen", alpha = 0.65, bins = 20)
```

From our histogram, we see that our average salary (in thousands)
roughly follows a normal distribution. Hence we will predict the raw
annual salary without further transformations.

Secondly, we visualize the job descriptions with a word cloud after
removing words that occur less than 40 times, and stopwords such as
"the", "he" etc. as these are common words that do not store any
informational value in our analysis. Below we show 100 of the most
common words.

```{r, echo = FALSE, warning = FALSE, fig.cap="\\label{fig:figs}Job description word cloud"}
# we create a function to visualize our text from "cleaned_text"
visualize_text <- function(x) {
  # x is a character vector the function will extract
  frequent_words <- termFreq(x) 
  # we remove stopwords
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())] 
  # generate a word cloud
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 40,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

### Document-term matrix

The document-term matrix (DTM) is a table reflecting the frequency of
each word where the column names represent words and the row names
represent documents, i.e., the reviews.

We create a DTM for our job descriptions.

```{r, echo = FALSE, comment = ""}
# we create a corpus and then a DTM
corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

# we find the original number of unique words
words_freq <- termFreq(D_clean$cleaned_text)

# we filter such that minimum frequency of each word is at least 40, and remove stopwords
frequent_words <- words_freq[words_freq >= 40]
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]

cat("Setting minimum word frequency to 40, we retain", length(frequent_words), "words out of the original", length(words_freq), "words.", "\n")
```

```{r, echo = FALSE, comment = ""}
# finally, we filter our DTM as well
dtm <- dtm[ , names(frequent_words)]

cat("Our document-term matrix consists of", nrow(dtm), "job descriptions against", ncol(dtm), 'words present in 
our vocab after removing stopwords.', '\n')
```

We create a new data frame for our DTM and produce a sample of it below.

```{r, echo = FALSE}
# convert DTM to a usable dataframe including our response variable
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y_salary = D_clean$`Avg Salary(K)`) %>%
  select(c(Y_salary, everything()))

knitr::kable(head(D_dtm[, c('data', 'machine', 'python', 'analytics', 'science')]), 
             caption = "A sample of our DTM.")
```

\newpage

# Modelling

## Feature selection using LASSO

We will use LASSO regularization to select features to prepare our data
for three of our models: Multiple Linear Regression, Random Forest, and
XGBoost. The last two models use their own methods of feature selection,
so we will not use features selected by LASSO for those two models, but
the entire dataset instead.

The LASSO procedure is as follows:

1.  Use cross-validation to find the best value of lambda (approx.
    0.359)
2.  Store our variables selected by LASSO
3.  Prepare our final dataset to be used for MLR, RF, and XGB

```{r, include = FALSE}
# We remove all output for the actual report.
# To show the output of our model and the lambda coefficient plot, remove 
# "include = FALSE" written above

set.seed(100)

# set values of lambda
lambda <- 10^seq(-1, 0 , length = 10)

# use cross-validation to find the best lambda for LASSO
lasso <- train(
  Y_salary ~., data = D_dtm, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso

# create function for plotting lambda against coefficients of variables
lambda_for_plotting <- 10^seq(from = -1, to = 1.2, length = 100)

# create a table consisting of our coefficients of variables and lambda values
lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
  as.matrix %>% t %>% as_tibble %>%
  mutate(lambda = lambda_for_plotting)


# plot lambda and coefficients
lasso_coefs %>%
  pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
  ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
  geom_line() + scale_x_log10() + theme(legend.position = "none")


# store our chosen variables according to the best lambda value
store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>% 
  select_if(function(col) !all(col == 0))

# keep all variables except our response variable and lambda
selected_var <- names(store[2:(ncol(store)-1)])

# prepare our final dataset for MLR, RF, and XGB
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
  mutate(Y_salary = D_dtm$Y_salary) %>%
  select(c(Y_salary, everything()))
```

```{r, echo = FALSE}
knitr::kable(head(D_final[, c('Y_salary', 'predictive', 'education', 'learn', 
                              'machine', 'experience', 'analytics', 'support')]), 
             caption = "A sample of our final dataset.")
```

We also prepare our training and test data to be used, and the
dimensions are as follows:

```{r, echo = FALSE, comment = ""}
set.seed(100)
ind <- runif(nrow(D_final)) < 0.8

# split our training and test data
train_data <- D_final[ind , ]
test_data <- D_final[!ind , ]

cat("Dimensions of the training set are", dim(train_data), "\n")
cat("Dimensions of the test set are", dim(test_data), "\n")
```

```{r, echo = FALSE, comment = ""}
# below is our training and test data set without Y_salary

train_data_nolabel <- train_data %>%
  select(-c(Y_salary))
test_data_nolabel <- test_data %>%
  select(-c(Y_salary))
```

From this, we can see that what started with over 10,000 unique words in
our job descriptions has been narrowed down to 354 words.

\newpage

## Models

### Multiple Linear Regression

The first model we use is Multiple Linear Regression, using the 354
features selected by LASSO.

```{r, echo = FALSE, comment = "", warning = FALSE}
mlr_mod <- lm(Y_salary ~ .,
               data = train_data)

mlr_rmse <- mlr_mod %>%
  predict(test_data) %>%
  caret::RMSE(test_data$Y_salary)

cat("RMSE:", round(mlr_rmse, 2), "\n")
```

After using cross-validation to find our lambda value for
regularization, our MLR model's RMSE value (rounded to two decimal
places) is 16.21, the lowest we will achieve in this project.

Below is a bar plot of the top 20 variables with the highest absolute
coefficients. The sign of their coefficients reflect a positive or
negative relationship with the predicted salary.

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}Top 20 variables according to absolute coefficients"}
var_coeff <- data.frame(coefficients(mlr_mod)) %>%
  arrange(desc(abs(coefficients(mlr_mod)))) %>%
  dplyr::slice(2:21)

ggplot(var_coeff, aes(x=coefficients.mlr_mod., 
                      y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
  geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
  xlab("Coefficients") + ylab("Keywords")
```

This plot shows that the presence of words such as "eligible" and
"molecules" in the job description will result in a higher predicted
salary, whereas words such as "streaming" and "wellness" in the job
description will lower the predicted salary.

\newpage

### Random Forest

The next step is to visualize our data using random forest decision
trees. We first train the data to obtain its optimal hyper-parameters.
We then perform grid search on the optimal hyper-parameter values to
minimize the out-of-bag error.

```{r, include = FALSE}
# again, remove "include = FALSE" to show the model produced.

set.seed(100)

#getting optimal hyper parameters
mod_rf <- train(Y_salary ~., data = D_final, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))

mod_rf
```

The optimal values of the hyper-parameters are:

```{r, echo = FALSE, comment = "", warning = FALSE}
knitr::kable(mod_rf$bestTune)
```

Afterwhich, we retrain the model with the selected hyper-parameters, and fit our
Random Forest model on the training data set.

```{r, include = FALSE}
# again, remove "include = FALSE" to see the output of the model

set.seed(100)

rfGrid <- expand.grid(mtry = 178,
                      min.node.size = 5,
                      splitrule = "extratrees")
                      
mod_rf_tune <- train(Y_salary ~., data = D_final, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))

mod_rf_tune
```

```{r, include = FALSE}
set.seed(100)

regr <- randomForest(x = train_data_nolabel, 
                     y = train_data$Y_salary, 
                     maxnodes = 100 , 
                     ntree = 1000)

predictions_rf <- predict(regr, test_data_nolabel)
```

Our RMSE for the Random Forest model (rounded to two decimal places) is
as shown below:

```{r, echo = FALSE, comment = "", warning = FALSE}
#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, predictions_rf), 2), "\n")
```

The bar graph below shows the top 20 most important variables in this
prediction. Based on the bar graph, the top 5 most important variables
are "machine", "analyst", "give", "phd", and "scientists".

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}Random forest top 20 most important variables"}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)

data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = importance, y = reorder(variable, importance))) + 
  geom_col(fill = "darkgreen", alpha = 0.7) + 
  xlab("Importance") + ylab("Keywords") 
```

\newpage

### XGBoost

Like random forests, gradient boosting machines are based on decision
trees. However, while random forest builds an ensemble of deep (i.e
complex) trees independent of one another, gradient boosting builds
shallow trees sequentially, where each tree learns and improves from the
previous tree. This is achieved by starting with a weak model and
subsequently boosting its performance by allowing each new tree to focus
on training data where the previous tree had the largest errors (or
residuals) in prediction. Each tree in the sequence is thus fitted
according to the residuals of the previous tree.

Moreover, it computes the second-order gradients, i.e. second partial
derivatives of the loss function, which provides more information about
the direction of gradients and how to get to the minimum of our loss
function while gradient boost uses the loss function of simple decision
tree model as a proxy to minimize the error of the overall model.

The objective function (loss function and regularization) at an
iteration $t$ that we need to minimize is as such:

$$
\mathcal{L}^{(t)} = \sum_{i=1}^n l(y_i, \hat y_i ^{(t-1)} + f_t (\mathbf{x}_i)) + \Omega(f_t)
$$

where $\Omega(f) = \gamma T + \frac{1}{2} \lambda \Vert w\Vert ^2$

The XGBoost objective is trained in an additive manner. $l$ is a
differentiable convex loss function that measures the difference between
prediction $\hat y_i$ and target $y_i$. The second term $\Omega$
penalizes the complexity of the model. From the equation, we greedily
add the $f_t$ that improves our model the most. Second-order
approximation can be used to quickly optimize the objective in the
general setting.

We create our XGBoost model from the caret library, using
hyper-parameters as shown below:

```{r, include = FALSE}
set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 650,
  eta = 0.21,
  max_depth = 3,
  gamma = 0.04,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

# To use for cross validation
xgbGrid_2 <- expand.grid(
  nrounds = seq(100, 650, length = 12),
  eta = seq(0.2, 0.3, length = 11),
  max_depth = seq(2, 10, length = 9),
  gamma = seq(0, 0.1, length = 11),
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y_salary ~.,
                   data = train_data,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)
```

in our model, we have used these hyperparameters:

1.  *gamma*: Pseudo-regularisation hyperparameter that controls the
    complexity of each tree.

2.  *nrounds*: Number of decision trees in the final model

3.  *eta*: Learning rate; determines the contribution of each tree on
    the final outcome and also how quickly the algorithm goes down the
    gradient descent.

4.  *max_depth*: Depth of each tree

5.  *min_child_weight*: Minimum number of observations in terminal
    nodes; controls complexity of the trees

6.  *colsample_bytree*: subsample of columns used for each tree
    (repeated for every tree)

7.  *subsample*: subsampling ratio of training data for growing trees to
    prevent over-fitting

The hyperparameters were tuned using 5-fold cross validation and grid
search to find the best model, and we arrived at the optimal values for
the hyperparameters.

Our chosen values for our hyper-parameters (through cross-validation)
and our RMSE for the XGBoost model (rounded to two decimal places) are
as shown below:

```{r, echo = FALSE, comment = "", warning = FALSE}
knitr::kable(data.frame(xgb_caret$finalModel$params) %>%
  dplyr::select(1:6) %>% 
  mutate(nrounds = 650))
```

```{r, echo = FALSE, comment = "", warning = FALSE}
pred_y_xgb = predict(xgb_caret, test_data)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, pred_y_xgb), 2), "\n")
```

Overall, XGBoost gave an RMSE value of 16.59.

We extracted the 20 most important features (words) from the XGBoost
model. The bar graph below shows the top 20 most important words in this
model. Based on the bar graph, the top 5 most important variables are
"machine", "give", "education", "analyst", and "phd".

```{r, echo = FALSE, comment = "", warning = FALSE}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)
```

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}XGBoost top 20 most important variables"}
ggplot(xgb_imp[1:20], aes(x = Gain, y = reorder(Feature, Gain))) + 
  geom_col(fill = "darkgreen", alpha = 0.7) + 
  xlab("Importance") + ylab("Keywords") 
```

\newpage

### Multivariate Adaptive Regression Splines (MARS)

We used multivariate adaptive regression splines (MARS) (Friedman 1991)
model here, it is an approach that automatically generates a piecewise
linear model that serves as an understandable stepping stone into
non-linearity after learning the notion of multiple linear regression.

By evaluating cutpoints (knots) similar to step functions, MARS offers a
practical method to capture the nonlinear relationships in the data.
This method evaluates every data point for every predictor as a knot and
builds a linear regression model using the candidate feature(s).

Consider non-linear, non-monotonic data where $Y = f(X)$. The MARS
method will initially search for a single point within a range of $X$
values where two distinct linear relationships between $Y$ and $X$
provide the lowest loss. The outcome is referred to as a hinge function
$h(x-a)$, where $a$ is the cutpoint value.

For example, if $a = 1$, our hinge function is $h(x-1)$ such that the
linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1 
\\ \beta_0 + \beta_1(x-1) \, ,& x > 1 \\ \end{cases}$$ After the first
knot is identified, the search for a second one begins, and it is
discovered at $x=2$. Now the linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1
\\ \beta_0 + \beta_1(x-1) \, ,& 1 < x < 2 
\\ \beta_0 + \beta_1(2-x) \, ,& x > 2 \\ \end{cases}$$ This process is
repeated until several knots are identified, leading to the creation of
a highly non-linear prediction equation. Even if using a lot of knots
could help us fit a particularly excellent relationship to our training
data, it might not perform well to unseen data. Once all of the knots
have been found, we may systematically eliminate knots that do not
significantly improve predictive accuracy. This is the pruning process,
and we may use cross-validation to determine the optimal number of
knots.

First, we divided the dataset into training and test data:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

ind_mars <- which(runif(nrow(D_dtm)) < 0.8)

train_mars <- D_dtm %>% dplyr::slice(ind_mars)
test_mars <- D_dtm %>% dplyr::slice(-ind_mars)

cat("Dimensions of the MARS training dataset are", dim(train_mars), "\n")
cat("Dimensions of the MARS test dataset are", dim(test_mars), "\n")
```

MARS model has two hyper-parameters: the maximum degree of interactions
and the number of terms retained in the final model. To achieve the
optimal combination of these tuning parameters, we conduct a grid search
that minimizes the error of prediction by building a grid with 30
different combinations of interaction complexity (degree) and the number
of terms to include in the final model (nprune).

```{r, include = FALSE}
marsgrid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 100, length.out = 20) %>% floor()
)
```

We performed required grid search by using 10-fold cross-validation to
determine our parameters:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

mars_cv <- train(Y_salary ~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid
)
```

```{r, echo = FALSE, comment = "", warning = FALSE}
knitr::kable(mars_cv$results %>%
  filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree))
```

The backwards elimination feature selection process used in MARS models
seeks for reductions in the generalized cross-validation (GCV) estimate
of error when each additional predictor is introduced to the model. The
variable importance is based on this overall reduction. MARS effectively
accomplishes automated feature selection since it will automatically
include and remove variables throughout the pruning phase.

After pruning, a predictor's significance value is 0 if it was never
used in any of the MARS basis functions in the final model. There are
only 9 features with importance values greater than 0, whereas the remaining
features obtain importance values of 0 since they were excluded
from the final model.

We also keep track of how the residual sums of squares (RSS) change when
terms are added. However, we noticed that there is minimum difference
between these two measures.

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}MARS top 20 most important variables"}
plot_GCV <- vip(mars_cv, num_features = 20, geom = "col", value = "gcv", 
                aesthetics = list(fill = "darkgreen", alpha = 0.7)) + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "col", value = "rss", 
                aesthetics = list(fill = "darkgreen", alpha = 0.7)) + ggtitle("RSS")

gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
```

We used the optimal hyper-parameters to train the model and 
calculated the RMSE:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

marsgrid_tuned <- expand.grid(
  degree = 1, 
  nprune = 11
)

mars_cv_tuned <- train(Y_salary ~., data = train_data,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid_tuned
)

pred_mars = predict(mars_cv_tuned, test_data)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, pred_mars), 2), "\n")
```

\newpage

### Partial Least Squares (PLS)

Partial Least Squares (PLS) is a common technique to analyse relative
importance when the data includes more predictors than observations. It
is an useful dimension reduction method which is similar with Principal
Component Analysis (PCA).

We do a regression against the response variable inside the narrower
space created by mapping the predictor variables to a smaller set of
variables. The response variable is not taken into account during the
dimension reduction process in PCA. PLS, on the other hand, seeks to
select newly mapped factors that best describe the response variable.

Similar to MARS, we divide the dataset into training and test data
first:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)
# 70/30 split
inTraining <- createDataPartition(D_dtm$Y_salary, p = .8, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]

cat("Dimensions of the MARS training dataset are", dim(train_pls), "\n")
cat("Dimensions of the MARS test dataset are", dim(test_pls), "\n")
```

The hyper-parameter for PLS model is the number of components used in
the model (ncomp). We conduct a grid search that minimize the prediction
error to achieve the optimal hyper-parameter. The grid search was
conducted by 10-fold cross-validation, and we used the optimal
hyper-parameter to train the model and calculated the RMSE as well:

```{r, include = FALSE}
set.seed(100)

plsGrid <- expand.grid(
  ncomp   = seq(1, 100, by = 1)
)

pls_cv <- train(
  Y_salary ~ .,
  data = train_pls,
  method = 'pls',
  metric = "RMSE",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid
)

pls_cv
```

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

plsGrid_tuned <- expand.grid(
  ncomp = 9
)

pls_cv_tuned <- train(Y_salary ~., data = train_pls,
  method = "pls",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid_tuned
)

pred_pls = predict(pls_cv_tuned, test_pls)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_pls$Y_salary, pred_pls), 2), "\n")
```

The barplots below show that 'addition', 'generate', 'framework' and
'characteristics' are positive predictors, while 'streaming',
'wellness', and 'molecules' are negative predictors:

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}PLS top 20 most important variables"}
coefficients_pls <- coef(pls_cv_tuned$finalModel)
sum.coef <- sum(sapply(coefficients_pls, abs))
coefficients_pls <- coefficients_pls * 100 / sum.coef
coefficients_pls <- sort(coefficients_pls[, 1 , 1])

coefficients_pls_df <- data.frame(coefficients_pls) %>%
  arrange(desc(abs(coefficients_pls))) %>%
  dplyr::slice(1:20)

ggplot(coefficients_pls_df, aes(x=coefficients_pls, 
                      y=reorder(rownames(var_coeff), coefficients_pls))) +
  geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
  xlab("Coefficients") + ylab("Key words")
```

\newpage

## Summary of results

Below we summarise the RMSE values (rounded to two decimal places) for
each model:

```{r, echo = FALSE, comment = "", warning = FALSE}
final_results <- data.frame(Model = c('MLR', 'Random Forest', 'XGBoost', 'MARS', 'PLS'),
                            RMSE = c(16.21, 21.59, 16.59, 30.00, 22.52))
knitr::kable(final_results, 
             caption = "Accuracy of models.")
```

# Conclusion

Recommendation engines are a commonly found solution applied to job
search portals, such as the Singapore government's national jobs portal,
MyCareersFuture. However, more can be done to bridge the gap between job
seekers and job satisfaction. In this project, we have produced models
that predict the expected average salary earned given a job description.
Our best performing model, Multiple Linear Regression, can be used to
help workers set expectations of salary based on keywords they value as
important in search of a job. This will help job seekers focus on
searching for their desired job role rather than focus on maximizing
salary earned, which will hopefully increase job satisfaction.

MLR also identified key terms such as "eligible", "mongodb", "upon",
"francisco", and "hypotheses". Some of these keywords may not seem to
make sense, and understanding the importance of these words is unclear.
Some of these words, on the other hand, give insight into areas that job
seekers can focus on, be it upskilling (for example, learning MongoDB),
or narrowing their job search to sectors such as actuarial science or
molecular chemistry in order to maximize their potential salary.

Based on RMSE, our best model uses Multiple Linear Regression, and it
has identified key terms that have a significant impact on data science
salary. However, Multiple Linear Regression does not identify the same
important features (words) as our other models. Comparing identified
important keywords between models was also not feasible within this
project as each models used different methods of ranking the importance
of variables. Different models also identified different keywords as
important. Further research is required to better understand the
difference between models and why they identify vastly different
features as important.

\newpage

# References

1.  Alexander, L. (2019). The importance of keywords in job ads. SEEK.
    Retrieved November 16, 2022, from
    <https://www.seek.com.au/employer/hiring-advice/the-importance-of-keywords-in-job-ads>
2.  Arora, P. (2022, August 29). What's keeping Singapore employees
    unhappy at work? - ETHRWorldSEA. HR News Southeast Asia. Retrieved
    November 7, 2022, from
    <https://hrsea.economictimes.indiatimes.com/news/employee-experience/whats-keeping-singapore-employees-unhappy-at-work/93833788>
3.  Chong, C. (2022, May 17). Nearly 1 in 3 workers in S'pore plans to
    change employers in first half of 2022: Survey. The Straits Times.
    Retrieved November 4, 2022, from
    <https://www.straitstimes.com/singapore/jobs/nearly-1-in-3-workers-in-spore-plan-to-change-employers-in-first-half-of-2022-survey>
4.  My Skills Future. (2021, June 21). 5 Crucial Skills You Need to
    Remain Employable in the Wake of Covid-19 \| Myskillsfuture.gov.sg.
    MySkillsFuture. Retrieved October 14, 2022, from
    <https://www.myskillsfuture.gov.sg/content/portal/en/career-resources/career-resources/education-career-personal-development/5-crucial-skills-you-need-to-remain-employable-during-covid.html>
5.  The Straits Times. (2021, December 22). It's a match: How
    skills-based hiring fits in the future of work. The Straits Times.
    Retrieved October 14, 2022, from
    <https://www.straitstimes.com/singapore/jobs/its-a-match-how-skills-based-hiring-fits-in-the-future-of-work>
6.  Tan, E. (2021, April 14). S'pore employers prioritise skills over
    education, experience: LinkedIn survey. The Straits Times. Retrieved
    October 14, 2022, from
    <https://www.straitstimes.com/singapore/jobs/singapore-employers-prioritise-skills-over-education-experience-linkedin-survey>

\
\
