---
title: "Data Science Salary Prediction"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# Team Humility

Chloe Neo Tze Ching Eng Jing Keat Goh Wei Lun Glenn Kaneko Yoshiki Tan
Wei Keong

## Abstract

Data mining is the process of sorting through large data sets to identify patterns and relationships, where we aim to gain meaningful insights through the use of algorithms to predict future trends in textual data. In this project, we aim to uncover key insights in data science salary by developing a model to effectively associate keywords in job descriptions to high paying data science jobs. For data cleaning, we filtered the keywords in job descriptions with a minimum occurrence of 40. 
We then use Least Absolute Shrinkage and Selection Operator (LASSO) for variable selection
The models used include Multiple Linear Regression (MLR), Random Forest, XGBoost and Neural Network Analysis. 
We then compare the individual model in terms of root mean squared error (rmse) and decide on the best model 


# 1. Introduction to the problem

## 1.1. Literature review

There is no doubt that COVID-19 has changed the employment landscape in
Singapore. A rapid shift towards digitalisation of businesses amidst a
recession means that Singaporeans have to keep pace with the change in
order to remain competitive at work (My Skills Future, 2021).

Singapore employers have prioritised skills over education (Tan, 2021).
Employers are starting to realise that new hires should be assessed
based on their existing skill sets --- instead of just their paper
qualifications and work history --- as there are various in-demand soft
skills transferable from one job to another. Other skills relevant to
the job may be learned through on-the-job training and development (The
Straits Times, 2021).

However, there is a gap between what individuals expect they will
require and what they have been taught in school among students. Just
27% of students say they are well equipped for future positions, while
22% say they are not at all prepared. According to a [2019
survey](https://www.cbi.org.uk/media/3841/12546_tess_2019.pdf) (Grimes,
2019), two out of five employers believe school and college graduates
are unprepared for employment. One-third say they are dissatisfied with
the quantity of relevant work experience young people have (EconoTimes,
2021).

The last decade has also shown us that whether or not one has a degree,
continual learning is imperative. In an age of ubiquitous disruption and
massive unpredictability, both employers and job seekers concede that
the knowledge and skills gained from a university degree can easily
become obsolete (Lim, 2021).

## 1.2. Objective

As the world grows more volatile, uncertain, complex and ambiguous,
graduates have to catch up with real-world needs and equip themselves
with the necessary skills. Considering that more employers are placing a
greater emphasis on individuals' skill sets and key traits than they are
on a piece of paper, this project aims to predict data scientist salary
based on the relevant variables in the dataset.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
names(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  # create a new variable containing only lowercase text from Job.Description
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) %>%
  # create a new variable containing length of job description texts
  mutate('desc_len' = sapply(D$Job.Description, nchar))

head(D_clean)
```

3.  Create a new variable to count the number of competitors a company
    has:

```{r}
# Function to convert "Competitors" variable from "-1" and strings to a count variable
my_strsplit <- function(string) {
  if (string == -1){
    output <- 0
  } else {
  output <- length(strsplit(toString(string), ", ")[[1]])
  }}

# Apply the function and create a new variable comp_count
D_clean$comp_count <- sapply(D_clean$Competitors, my_strsplit)

# View head of variables 'Competitors' and 'comp_count'
head(D_clean[, c('Competitors', 'comp_count')])
```

4.  Variables Selection

After removing unnecessary columns, our data contains 33 variables for
analysis, and "Avg.Salary.K." will be our response variable.

```{r}
#remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(Rating, Avg.Salary.K., Age, desc_len, comp_count, cleaned_text))
head(D_clean)
```

### 2.2.2. Data visualizations

Below is a summary of our numeric variables:

```{r}
summary(D_clean%>%select(c(Rating, Avg.Salary.K., Age, desc_len, comp_count)))
```

#### 2.2.2.1. Numerical variables data visualizations

1.  Histograms

Histogram of rating:

```{r}
avg <- D_clean %>% filter(Rating >= 0)
D_clean$Rating[D_clean$Rating == -1] <- mean(avg$Rating)

ggplot(D_clean, aes(x=Rating)) +
  geom_histogram(fill = "blue", alpha = 0.7)
```

-   Companies without rating are given a -1 rating. Since the rest of
    the ratings follow a normal distribution, we replace the ratings
    with a mean value instead.

Histogram of average salary in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram(fill = "blue", alpha = 0.7)
```

Histogram of age of companies:

```{r}
ggplot(D_clean, aes(x=Age)) + 
  geom_histogram(fill = "blue", alpha = 0.7)
```

Histogram of job description text length:

```{r}
ggplot(D_clean, aes(x=desc_len)) +
  geom_histogram(fill = "blue", alpha = 0.7)
```

2.  Boxplots

Boxplot of salaries

```{r}
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Rating:

```{r}
ggplot(D_clean, aes(y=Rating)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Age:

```{r}
ggplot(D_clean, aes(y=Age)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Job Description Length:

```{r}
ggplot(D_clean, aes(y=desc_len)) + 
  geom_boxplot(fill = "blue", alpha = 0.5)
```

4.  Corrplot

```{r}
D_num <- D_clean %>%
  select(c(Avg.Salary.K., Rating, Age, desc_len, comp_count)) 

D_num %>%
  rename('comp' = 'comp_count', 'desc' = 'desc_len', 'avg_S' = 'Avg.Salary.K.') %>%
  ggcorr(palette = "RdBu", label = TRUE)
```

```{r}
D_num %>% 
  select_if(is.numeric) %>%
  ggpairs()
```

```{r}
D_num %>%
  select_if(is.numeric) %>%
  cor %>% round(3)
```

## 2.3. Feature engineering

### 2.3.1. Identifying variables

Perhaps the best choice now would be to narrow down our variables
selected.

We can narrow down our variables to only numerical variables and job
description.

### 2.3.2. Variable selection

First we generate a word cloud for our Job Description variable

```{r}
visualize_text <- function(x) {
  # x is a character vector
  # the function will extract
  frequent_words <- termFreq(x)
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 0,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

s/n: We should take a look at what words appear if the min. occurrence
is 50. i.e. is 50 a good threshold?

For now, we set the threshold as 40

```{r}
library(tm)
library(wordcloud)

corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]
length(frequent_words)

frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
dtm <- dtm[ , names(frequent_words)]
dim(dtm)
```

\^ this is total number of job descriptions (536) by the words present
in our vocab (from the job descriptions) (932)

Next, we create a new dataframe for our DTM

```{r}
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y = D_clean$Avg.Salary.K.)

head(D_dtm)
```

# 3. Objectives

## 3.1. Modelling

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

```{r}
# Libraries used:
library(tidyverse)
library(stargazer)
library(caret)

set.seed(100)
ind <- runif(nrow(D_dtm)) < 0.8

train_mlr <- D_dtm[ind , ]
test_mlr <- D_dtm[!ind , ]

cat("Dimensions of the training set are", dim(train_mlr), "\n")
cat("Dimensions of the test set are", dim(test_mlr), "\n")
```

```{r}
lambda <- 10^seq(-1, 0 , length = 20)
lambda
```

```{r}
set.seed(100)

lasso <- train(
  Y ~., data = train_mlr, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso
```

```{r}
store <- coef(lasso$finalModel, lasso$bestTune$lambda)

##Go through each row and determine if a value is zero
row_sub = apply(store, 1, function(row) all(row !=0 ))
##Subset as usual
D_coeff <- data.frame(store[row_sub,])

D_coeff %>% 
  arrange(desc(abs(D_coeff$store.row_sub...)))

rownames(D_coeff)[2:293]
```

```{r}
set.seed(100)

x <- train_mlr %>% select(contains(rownames(D_coeff)[2:293]) | Y)
head(x)
dim(x)

lasso_final <- lm(Y ~ ., data = x)

stargazer(lasso_final, type = "text")
```

```{r}
pred_y = predict(lasso_final, test_mlr)
caret::RMSE(test_mlr$Y, pred_y) #rmse
```

```{r}
library(leaps)
subset_mod <- regsubsets(Y ~ . , data = train_dtm,
                         nvmax = ncol(train_dtm) - 1,
                         method = "forward")

sum_subset_mod <- summary(subset_mod)
t(sum_subset_mod$outmat)[ , 1:6]
```

```{r}
coef(subset_mod, 1:6)
```

```{r}
sum_subset_mod$cp
```

```{r}
criteria <- tibble(
  k = 1:length(sum_subset_mod$cp),
  Cp = sum_subset_mod$cp,
  BIC = sum_subset_mod$bic,
  "1 - Adj.R2" = 1 - sum_subset_mod$adjr2
)

criteria
```

```{r}
ggplot(data = criteria, aes(x = k, y = BIC)) +
  geom_line() + geom_point() +
  geom_vline(aes(xintercept = k[which.min(BIC)]), 
             color = "blue", linetype = "dashed")
```

```{r}
variable_selection_matrix <- sum_subset_mod$which
colnames(variable_selection_matrix)[1] <- "Y"
variable_selection_matrix[1:5, 1:5]
```

```{r}
train_dtm %>%
  select(which(variable_selection_matrix[3 , ])) %>%
  head

train_subset <- train_dtm %>%
  select(which(variable_selection_matrix[3 , ]))
```

```{r}
caret_mod <- train(Y ~ . , train_subset,
      method = "lm", trControl = trainControl(method = "cv", number = 5))
caret_mod
```

```{r}
cv_error <- function(vars_to_select) {
  train_subset <- train_dtm %>% select(which(vars_to_select))
  caret_mod <- train(Y ~ . , train_subset,
      method = "lm", trControl = trainControl(method = "cv", number = 5))
  caret_mod$results$RMSE
}

cv_error(variable_selection_matrix[ 3, ])
```

```{r}
CV_errors <- apply(variable_selection_matrix, 1, cv_error)
CV_errors
```

```{r}
criteria$CV <- CV_errors
criteria
```

```{r}
ggplot(data = criteria, aes(x = k, y = CV)) +
  geom_line() + geom_point() +
  geom_vline(aes(xintercept = k[which.min(CV)]), 
             color = "blue", linetype = "dashed")
```

```{r}
criteria$k[which.min(criteria$CV)]
```

```{r}
criteria %>% filter(k == 6)
```

\

\

```{r}
k <- 10

set.seed(100)

add_folds_to_data <- function(dataset, k) {
  N <- nrow(dataset)
  folds <- ceiling((1:N)/(N/k))
  dataset %>% 
    mutate(fold = sample(folds, N))
}

train_mlr <- train_mlr %>%
  add_folds_to_data(k)

head(train_mlr)
```

Then we check to see if our data was properly split

```{r}
table(train_mlr$fold)
```

We build functions to train models of different degrees and to calculate
RMSE

```{r}
train_poly_model <- function(deg, dataset = train_mlr) {
  mlr_mod <- lm(Avg.Salary.K. ~ polym(comp_count, desc_len, Age, Rating, 
                                      degree = deg, raw = TRUE),
               data = dataset)
}

error_rate <- function(mlr_mod, dataset = test_mlr) {
  mlr_mod %>%
    predict(dataset) %>%
    rmse(dataset$Avg.Salary.K.)
}

train_and_validate <- function(deg, train = train_mlr, test = test_mlr) {
  train_poly_model(deg, train) %>%
    error_rate(test)
}
```

Create a function to apply different degrees to our previous functions,
and compile them altogether to create a data frame consisting of CV
error and test error values

```{r setup. warning = FALSE}
cv_and_test_error <- function(d, K = k) {
  # K is the number of folds, the default value is whatever is specified above
  # when we assigned k
  cv_error <- 1:K %>% sapply(
    function(i) train_and_validate(d, train_mlr %>% filter(fold != i),
                                 train_mlr %>% filter(fold == i))
    ) %>% mean
  
  test_error <- train_and_validate(d, train_mlr, test_mlr)
  
  c(deg = d, CV = cv_error, TEST = test_error)
}

all_degrees <- 1:5
final_errors <- sapply(all_degrees, cv_and_test_error) %>% t %>%
  as.data.frame
```

```{r}
final_errors
```

We do only up to degree 5 because on further testing, greater degrees
resulted in gross increases in CG and test errors, which indicates gross
overfitting. Cross validation has shown that a degree 1 polynomial works
best.

We should perhaps perform regularization first to select continuous
variables via cross-validation, and then apply cross-validation again to
select the most appropriate degree.

#### 3.1.1.2. Random forest

Next, we want to visualise our data through the use of decision trees.
However, decision trees alone have a high variance which can cause the
trees trained on different parts of the dataset to look very different.
Hence, we will introduce an ensemble method to reduce the variance.
Random forest is chosen over a bagged ensemble as there may exist one or
more very strong predictors in our dataset, causing other predictors to
not have a chance to be included. This will cause the predictions of
trees to be highly correlated and adding more trees will not reduce the
variance. Variable selection can also be done by looking at the variable
importance of the random forest.

We performed grid search with oob (out-of-bag error) while tuning the
random forest to select the optimal values of the hyper-parameters try
(number of variables) and min.node.size (minimal node size):

```{r}
#getting optimal hyper parameters
mod_rf <- train(Y~., data = D_dtm, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))
mod_rf
```

The optimal values of the hyper-parameters are:

```{r}
mod_rf$bestTune
```

Now, we retrain the model with the selected hyperparameters to produce
the following output:

```{r}
rfGrid <- expand.grid(mtry = 43,
                      min.node.size = 5,
                      splitrule = "variance")
                      
mod_rf_tune <- train(Y~., data = D_dtm, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
```

Then, we split our data into train and test data sets:

```{r}
#splitting into train and test data
X <- D_dtm %>%
  select(-c(Y))
y <- D_dtm$Y

index <- createDataPartition(y, p=0.7, list=FALSE)
X_train <- X[index, ]
X_test <- X[-index, ]
Y_train <- y[index]
Y_test<- y[-index]

cat("Dimensions of the training set are", dim(X_train), "\n")
cat("Dimensions of the test set are", dim(X_test), "\n")
```

Next, we perform random forest on the train data set

```{r}
#random forest regression
regr <- randomForest(x = X_train, y = Y_train , maxnodes = 100 , ntree = 1000)
predictions <- predict(regr, X_test)

result <- X_test
result['Y'] <- Y_test
result['predictions'] <- predictions

head(result)
```

The graph below shows the data between Actual Average Salary vs.
Predicted Average Salary

```{r}
#plot graph to compare actual vs. predicted
ggplot(  ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Actual Average Salary", y = "Predicted Average Salary", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
```

```{r}
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , Y_test) ))
print(paste0('RMSE: ' ,caret::postResample(predictions , Y_test)['RMSE'])) #21.48
```

The bar graph below shows the top 20 most important variables in this
prediction. Based on the bar graph, the top 5 most important variables are ..

```{r}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)
var_importance
data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() + 
  xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
```

#### 3.1.1.3. XGBoost

First we split our data into test and training sets

```{r}
set.seed(100)

ind <- runif(nrow(D_dtm)) < 0.8

train_dtm <- D_dtm[ind , ]
test_dtm <- D_dtm[!ind , ]

cat("Dimensions of the training set are", dim(train_dtm), "\n")
cat("Dimensions of the test set are", dim(test_dtm), "\n")
```

Then we create our XGBoost model from caret, using hyperparameters as
shown below:

```{r}
library(caret)

set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 350,
  eta = 0.1,
  max_depth = 4,
  gamma = 0.02,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgbGrid_2 <- expand.grid(
  nrounds = seq(40, 100, length = 7),
  eta = seq(0.1, 0.6, length = 5),
  max_depth = seq(11, 20, length = 10),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y ~.,
                   data = train_dtm,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)

xgb_caret
```

```{r}
pred_y = predict(xgb_caret, test_dtm)

#measure prediction accuracy
caret::MAE(test_dtm$Y, pred_y) #mae
caret::RMSE(test_dtm$Y, pred_y) #rmse
```

```{r}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)

xgb_imp[1:20]

xgb.plot.importance(xgb_imp[1:20])
```

#### 3.1.1.4. Multivariate Adaptive Regression Splines (MARS)

We used multivariate adaptive regression splines (MARS) (Friedman 1991)
model here, it is an approach that automatically generates a piecewise
linear model that serves as an understandable stepping stone into
non-linearity after learning the notion of multiple linear regression.

By evaluating cutpoints (knots) similar to step functions, MARS offers a
practical method to capture the nonlinear relationships in the data.
This method evaluates every data point for every predictor as a knot and
builds a linear regression model using the candidate feature(s).

Consider non-linear, non-monotonic data where $Y = f(X)$. The MARS
method will initially search for a single point within a range of $X$
values where two distinct linear relationships between $Y$ and $X$
provide the lowest loss. The outcome is referred to as a hinge function
$h(x-a)$, where $a$ is the cutpoint value.

For example, if $a = 1$, our hinge function is $h(x-1)$ such that the
linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1 
\\ \beta_0 + \beta_1(x-1) \, ,& x > 1 \\ \end{cases}$$
After the first knot is identified, the search for a second one begins,
and it is discovered at $x=2$. Now the linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1
\\ \beta_0 + \beta_1(x-1) \, ,& 1 < x < 2 
\\ \beta_0 + \beta_1(2-x) \, ,& x > 2 \\ \end{cases}$$
This process is repeated until several knots are identified, leading to
the creation of a highly non-linear prediction equation. Even if using a
lot of knots could help us fit a particularly excellent relationship to
our training data, it might not perform well to unseen data. Once all of
the knots have been found, we may systematically eliminate knots that do
not significantly improve predictive accuracy. This is pruning process,
and we may use cross-validation to determine the optimal number of
knots.

We will use the following packages. First of all, we divided the dataset
into training dataset and test dataset:

```{r}
library(dplyr)
library(ggplot2)
library(caret) 
library(earth)
library(vip)

set.seed(8888)

ind_mars <- which(runif(nrow(D_dtm)) < 0.7)

train_mars <- D_dtm %>% slice(ind_mars)
test_mars <- D_dtm %>% slice(-ind_mars)

cat("Dimensions of the training dataset are", dim(train_mars), "\n")
cat("Dimensions of the test dataset are", dim(test_mars), "\n")
```

MARS model have two hyperparameters: the maximum degree of interactions
and the number of terms retained in the final model. To achieve the
optimal combination of these tuning parameters, we must conduct a grid
search that minimize the error of prediction.

Here, we built up a grid with 30 different combinations of interaction
complexity (degree) and the number of terms to include in the final
model (nprune).

```{r}
marsgrid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 100, length.out = 20) %>% floor()
)
```

We performed required grid search by using 10-fold cross-validation:

```{r}
set.seed(9999)

mars_cv <- train( Y~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid
)

mars_cv$bestTune #nprune = 16, degree = 2

mars_cv$results %>%
  filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree)

ggplot(mars_cv)
```

The backwards elimination feature selection process used in MARS models
seeks for reductions in the generalized cross-validation (GCV) estimate
of error when each additional predictor is introduced to the model. The
variable importance is based on this overall reduction. MARS effectively
accomplishes automated feature selection since it will automatically
include and remove variables throughout the pruning phase.

After pruning, a predictor's significance value is 0 if it was never
used in any of the MARS basis functions in the final model. There are
only 17 features have importance values greater than 0, whereas the
other features all have importance values of zero since they were
excluded from the final model.

We also kept track of how the residual sums of squares (RSS) change when
terms are added. However, we noticed that there is no much difference
between these two measures.

```{r}
plot_GCV <- vip(mars_cv, num_features = 20, geom = "point", value = "gcv") + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "point", value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
```

We used the optimal hyperparameters to train the model and then
calculated the RMSE:

```{r}
set.seed(7777)

marsgrid_tuned <- expand.grid(
  degree = 2, 
  nprune = 16
)

mars_cv_tuned <- train( Y~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid_tuned
)

pred_mars = predict(mars_cv_tuned, test_mars)

print(caret::MAE(test_mars$Y, pred_mars)) #23.6061
print(caret::RMSE(test_mars$Y, pred_mars)) #34.19323
```

#### 3.1.1.5. Ridge Regression

```{r}
library(glmnet)

ind <- runif(nrow(D_dtm)) < 0.8
train_data = D_dtm[ind, ]
test_data = D_dtm[!ind, ]

#fit ridge regression model
model <- glmnet(x = train_data, y = train_data$Y, alpha = 0)
summary(model)
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
lambda = 10^seq(from = 1, to = 4, length = 100)
ridge <- train(Y ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("scale")
)
ridge
```

```{r}
#find optimal lambda value that minimizes test MSE
best_lambda <- ridge$lambda.min
best_lambda
```

```{r}
#find coefficients of best model
best_model <- glmnet(x = train_data, y = train_data$Y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = train_data)
```

```{r}
#find SSE
sse <- sum((y_predicted - Y_test)^2)
n <- nrow(y_predicted)

#find RMSE
rmse <- sqrt((1/n) / sse)
rmse
```

#### 3.1.1.6. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a
modification of linear regression, as there is an extra regularization
term in its loss function as compared to that of linear regression. This
is to lower the complexity of the model by limiting the sum of the
absolute values of the coefficients. Meanwhile, elastic net regression
combines the properties of both linear and LASSO regressions, and there
are two hyperparameters, namely β and α. Here is the loss function of
elastic net regression: $$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$ We will need to choose the optimum values for these two
hyperparameters before generating predictions.

##### 3.1.1.6.1. Elastic Net

```{r}
elasticnet <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 5))

elasticnet
```

##### 3.1.1.6.2. LASSO

```{r}
lambda <- 10^seq(-3, 0, length = 20)
lambda
```

```{r}
lasso <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda), preProcess = c("scale"))

lasso
```

```{r}
lasso$bestTune
```

#### 3.1.1.7. Partial Least Squares (PLS)

Partial Least Squares (PLS) is a common technique to analyse relative importance when the data includes more predictors than observations. It is an useful dimension reduction method which is similar with principal component analysis (PCA).

We do a regression against the response variable inside the narrower space created by mapping the predictor variables to a smaller set of variables. The response variable is not taken into account during the dimension reduction process in PCA. PLS, on the other hand, seeks to select newly mapped factors that best describe the response variable.

Below are the required packages. We divided the dataset into training dataset and test dataset first:

```{r}
library(pls)

set.seed(77777)
# 80/20 split
inTraining <- createDataPartition(D_dtm$Y, p = .80, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]
```

The hyperparameter for PLS model is the number of components used in the model (ncomp) .We conduct a grid search that minimize the prediction error to achieve the optimal hyperparameter. The grid search was conducted by 10-fold cross-validation:

```{r}
set.seed(88888)

plsGrid <- expand.grid(
  ncomp   = seq(1, 100, by = 1)
)

pls_cv <- train(
  Y ~ .,
  data = train_pls,
  method = 'pls',
  metric = "RMSE",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid
)

pls_cv

pls_cv$bestTune #6

plot(pls_cv)
```

We used the optimal hyperparameter to train the model and calculated the RMSE as well:

```{r}
set.seed(99999)

plsGrid_tuned <- expand.grid(
  ncomp = 6
)

pls_cv_tuned <- train( Y~., data = train_pls,
  method = "pls",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid_tuned
)

pred_pls = predict(pls_cv_tuned, test_pls)

print(caret::MAE(test_pls$Y, pred_pls)) #19.02091
print(caret::RMSE(test_pls$Y, pred_pls)) #27.3268
```

The barplots below show that 'credit', 'lead', 'mission', 'actuarial' and 'scientists' are positive predictors, while 'support', 'use', 'project', 'solutions' and 'food' are negative predictors:
```{r}
coefficients = coef(pls_cv_tuned$finalModel)
sum.coef = sum(sapply(coefficients, abs))
coefficients = coefficients * 100 / sum.coef
coefficients = sort(coefficients[, 1 , 1])

barplot(tail(coefficients, 5))
barplot(head(coefficients, 5))
```

### 3.1.2. Beyond the syllabus

#### 3.1.2.1. Shapley Value Regression

Shapley Value Regression is a technique used to determine the relative
importance of predictor variables in linear regression. It derives from
game theory and its purpose is to evaluate the worth of each player's
input over all possible combinations of players. In the study, it aims
to identify main variables predicting data scientists' salaries and to
avoid the problem of high correlation between variables.

```{r}
set.seed(1234)
ind <- runif(nrow(D_clean)) < 0.7

train_data <- D_clean[ind , ] 
test_data <- D_clean[!ind , ]

avg_S = train_data$Avg.Salary.K
rating = train_data$Rating
age = train_data$Age
comp = train_data$comp_count

lmsalary = lm(avg_S ~ comp + age  + rating)
summary(lmsalary)

library(relaimpo)
calc.relimp(lmsalary, type=list('lmg'), rela=T)

library(ggplot2)
plot(lm(avg_S ~ comp + age  + rating))

#relative importance metrics
names <- c("comp", "age", "rating")
count = c(0.2792880, 0.5205512, 0.2001608)

barplot(count,names.arg=names,xlab="Explanatory Variables",ylab="Relative Importance of Variables")
```

From the results, Age of the company is the most important predictor
variable. It accounts for approximately 52.1% of the average salary.
Established companies are more stable and have funds to provide higher
salaries as compared to start-ups. In contrast, other variables such as
number of competitors and rating only account for 27.9% and 20% of the
average salary. "comp" might be slightly higher than rating because
companies have to offer more attractive salaries to employees so they
will be less likely to job-hop to their competitor companies. \####
3.1.2.2. Theory behind the model

## 3.2. Summary of results

# 4. Conclusion
