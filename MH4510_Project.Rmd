---
title: "Data Science Salary Prediction"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# Team Humility

Chloe Neo Tze Ching Eng Jing Keat Goh Wei Lun Glenn Kaneko Yoshiki Tan
Wei Keong

## Abstract

How to use Github:

1.  Open GitHub Desktop app
2.  Pull from origin to receive the most updated .Rmd file
3.  Make your edits on the MH4510_Project.Rmd file
4.  Save your edits in the same file
5.  Add descriptions for changes made
6.  Commit changes
7.  Push changes

Note that pulling the most update project file will override the project
file existing in your project folder. You may either create a fork to
save your changes, or simply create a separate .Rmd file to work on
personally before copying your edits to the main project file.

# 1. Introduction to the problem

## 1.1. Literature review

There is no doubt that COVID-19 has changed the employment landscape in
Singapore. A rapid shift towards digitalisation of businesses amidst a
recession means that Singaporeans have to keep pace with the change in
order to remain competitive at work (My Skills Future, 2021).

Singapore employers have prioritised skills over education (Tan, 2021).
Employers are starting to realise that new hires should be assessed
based on their existing skill sets --- instead of just their paper
qualifications and work history --- as there are various in-demand soft
skills transferable from one job to another. Other skills relevant to
the job may be learned through on-the-job training and development (The
Straits Times, 2021).

However, there is a gap between what individuals expect they will
require and what they have been taught in school among students. Just
27% of students say they are well equipped for future positions, while
22% say they are not at all prepared. According to a [2019
survey](https://www.cbi.org.uk/media/3841/12546_tess_2019.pdf) (Grimes,
2019), two out of five employers believe school and college graduates
are unprepared for employment. One-third say they are dissatisfied with
the quantity of relevant work experience young people have (EconoTimes,
2021).

The last decade has also shown us that whether or not one has a degree,
continual learning is imperative. In an age of ubiquitous disruption and
massive unpredictability, both employers and job seekers concede that
the knowledge and skills gained from a university degree can easily
become obsolete (Lim, 2021).

## 1.2. Objective

As the world grows more volatile, uncertain, complex and ambiguous,
graduates have to catch up with real-world needs and equip themselves
with the necessary skills. Considering that more employers are placing a
greater emphasis on individuals' skill sets and key traits than they are
on a piece of paper, this project aims to predict data scientist salary
based on the relevant variables in the dataset.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(neuralnet)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(keras)
library(tensorflow)

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
names(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  # create a new variable containing only lowercase text from Job.Description
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) %>%
  # create a new variable containing length of job description texts
  mutate('desc_len' = sapply(D$Job.Description, nchar))

head(D_clean)
```

3.  Create a new variable to count the number of competitors a company
    has:

```{r}
# Function to convert "Competitors" variable from "-1" and strings to a count variable
my_strsplit <- function(string) {
  if (string == -1){
    output <- 0
  } else {
  output <- length(strsplit(toString(string), ", ")[[1]])
  }}

# Apply the function and create a new variable comp_count
D_clean$comp_count <- sapply(D_clean$Competitors, my_strsplit)

# View head of variables 'Competitors' and 'comp_count'
head(D_clean[, c('Competitors', 'comp_count')])
```

4.  Variables Selection

After removing unnecessary columns, our data contains 33 variables for
analysis, and "Avg.Salary.K." will be our response variable.

```{r}
#remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(Rating, Avg.Salary.K., Age, desc_len, comp_count, cleaned_text))
head(D_clean)
```

### 2.2.2. Data visualizations

Below is a summary of our numeric variables:

```{r}
summary(D_clean%>%select(c(Rating, Avg.Salary.K., Age, desc_len, comp_count)))
```

#### 2.2.2.1. Numerical variables data visualizations

1.  Histograms

Histogram of rating:

```{r}
avg <- D_clean %>% filter(Rating >= 0)
D_clean$Rating[D_clean$Rating == -1] <- mean(avg$Rating)

ggplot(D_clean, aes(x=Rating)) +
  geom_histogram(fill = "blue", alpha = 0.7)
```

-   Companies without rating are given a -1 rating. Since the rest of
    the ratings follow a normal distribution, we replace the ratings
    with a mean value instead.

Histogram of average salary in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram(fill = "blue", alpha = 0.7)
```

Histogram of age of companies:

```{r}
ggplot(D_clean, aes(x=Age)) + 
  geom_histogram(fill = "blue", alpha = 0.7)
```

Histogram of job description text length:

```{r}
ggplot(D_clean, aes(x=desc_len)) +
  geom_histogram(fill = "blue", alpha = 0.7)
```

2.  Boxplots

Boxplot of salaries

```{r}
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Rating:

```{r}
ggplot(D_clean, aes(y=Rating)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Age:

```{r}
ggplot(D_clean, aes(y=Age)) +
  geom_boxplot(fill = "blue", alpha = 0.5)
```

Boxplot of Job Description Length:

```{r}
ggplot(D_clean, aes(y=desc_len)) + 
  geom_boxplot(fill = "blue", alpha = 0.5)
```

4.  Corrplot

```{r}
D_num <- D_clean %>%
  select(c(Avg.Salary.K., Rating, Age, desc_len, comp_count)) 

D_num %>%
  rename('comp' = 'comp_count', 'desc' = 'desc_len', 'avg_S' = 'Avg.Salary.K.') %>%
  ggcorr(palette = "RdBu", label = TRUE)
```

```{r}
D_num %>% 
  select_if(is.numeric) %>%
  ggpairs()
```

```{r}
D_num %>%
  select_if(is.numeric) %>%
  cor %>% round(3)
```

## 2.3. Feature engineering

### 2.3.1. Identifying variables

Perhaps the best choice now would be to narrow down our variables
selected.

We can narrow down our variables to only numerical variables and job
description.

### 2.3.2. Variable selection

First we generate a word cloud for our Job Description variable

```{r}
visualize_text <- function(x) {
  # x is a character vector
  # the function will extract
  frequent_words <- termFreq(x)
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 0,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

s/n: We should take a look at what words appear if the min. occurrence
is 50. i.e. is 50 a good threshold?

For now, we set the threshold as 40

```{r}
library(tm)
library(wordcloud)

corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]
length(frequent_words)

frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
dtm <- dtm[ , names(frequent_words)]
dim(dtm)
```

\^ this is total number of job descriptions (536) by the words present
in our vocab (from the job descriptions) (932)

Next, we create a new dataframe for our DTM

```{r}
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y = D_clean$Avg.Salary.K.)

head(D_dtm)
```

# 3. Objectives

## 3.1. Modelling

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

```{r}
# Libraries used:
library(tidyverse)
library(stargazer)
library(caret)

set.seed(100)
ind <- runif(nrow(D_dtm)) < 0.8

train_mlr <- D_dtm[ind , ]
test_mlr <- D_dtm[!ind , ]

cat("Dimensions of the training set are", dim(train_mlr), "\n")
cat("Dimensions of the test set are", dim(test_mlr), "\n")
```

```{r}
lambda <- 10^seq(-1, 0 , length = 20)
lambda
```

```{r}
set.seed(100)

lasso <- train(
  Y ~., data = train_mlr, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso
```

```{r}
store <- coef(lasso$finalModel, lasso$bestTune$lambda)

##Go through each row and determine if a value is zero
row_sub = apply(store, 1, function(row) all(row !=0 ))
##Subset as usual
D_coeff <- data.frame(store[row_sub,])

D_coeff %>% 
  arrange(desc(abs(D_coeff$store.row_sub...)))

rownames(D_coeff)[2:293]
```

```{r}
set.seed(100)

x <- train_mlr %>% select(contains(rownames(D_coeff)[2:293]) | Y)
head(x)
dim(x)

lasso_final <- lm(Y ~ ., data = x)

stargazer(lasso_final, type = "text")
```

```{r}
pred_y = predict(lasso_final, test_mlr)
caret::RMSE(test_mlr$Y, pred_y) #rmse
```

\

```{r}
k <- 10

set.seed(100)

add_folds_to_data <- function(dataset, k) {
  N <- nrow(dataset)
  folds <- ceiling((1:N)/(N/k))
  dataset %>% 
    mutate(fold = sample(folds, N))
}

train_mlr <- train_mlr %>%
  add_folds_to_data(k)

head(train_mlr)
```

Then we check to see if our data was properly split

```{r}
table(train_mlr$fold)
```

We build functions to train models of different degrees and to calculate
RMSE

```{r}
train_poly_model <- function(deg, dataset = train_mlr) {
  mlr_mod <- lm(Avg.Salary.K. ~ polym(comp_count, desc_len, Age, Rating, 
                                      degree = deg, raw = TRUE),
               data = dataset)
}

error_rate <- function(mlr_mod, dataset = test_mlr) {
  mlr_mod %>%
    predict(dataset) %>%
    rmse(dataset$Avg.Salary.K.)
}

train_and_validate <- function(deg, train = train_mlr, test = test_mlr) {
  train_poly_model(deg, train) %>%
    error_rate(test)
}
```

Create a function to apply different degrees to our previous functions,
and compile them altogether to create a data frame consisting of CV
error and test error values

```{r setup. warning = FALSE}
cv_and_test_error <- function(d, K = k) {
  # K is the number of folds, the default value is whatever is specified above
  # when we assigned k
  cv_error <- 1:K %>% sapply(
    function(i) train_and_validate(d, train_mlr %>% filter(fold != i),
                                 train_mlr %>% filter(fold == i))
    ) %>% mean
  
  test_error <- train_and_validate(d, train_mlr, test_mlr)
  
  c(deg = d, CV = cv_error, TEST = test_error)
}

all_degrees <- 1:5
final_errors <- sapply(all_degrees, cv_and_test_error) %>% t %>%
  as.data.frame
```

```{r}
final_errors
```

We do only up to degree 5 because on further testing, greater degrees
resulted in gross increases in CG and test errors, which indicates gross
overfitting. Cross validation has shown that a degree 1 polynomial works
best.

We should perhaps perform regularization first to select continuous
variables via cross-validation, and then apply cross-validation again to
select the most appropriate degree.

### Ridge Regression

```{r}
library(glmnet)

ind <- runif(nrow(D_dtm)) < 0.8
train_data = D_dtm[ind, ]
test_data = D_dtm[!ind, ]

#fit ridge regression model
model <- glmnet(x = train_data, y = train_data$Y, alpha = 0)
summary(model)
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
lambda = 10^seq(from = 1, to = 4, length = 100)
ridge <- train(Y ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("scale")
)
ridge
```

```{r}
#find optimal lambda value that minimizes test MSE
best_lambda <- ridge$lambda.min
best_lambda
```

```{r}
#find coefficients of best model
best_model <- glmnet(x = train_data, y = train_data$Y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = train_data)
```

```{r}
#find SSE
sse <- sum((y_predicted - Y_test)^2)
n <- nrow(y_predicted)

#find RMSE
rmse <- sqrt((1/n) / sse)
rmse
```

#### 3.1.1.2. Random forest/deep forest (gcforest)/XGBoost

Next, we want to visualise our data through the use of decision trees.
However, decision trees alone have a high variance which can cause the
trees trained on different parts of the dataset to look very different.
Hence, we will introduce an ensemble method to reduce the variance.
Random forest is chosen over a bagged ensemble as there may exist one or
more very strong predictors in our dataset, causing other predictors to
not have a chance to be included. This will cause the predictions of
trees to be highly correlated and adding more trees will not reduce the
variance. Variable selection can also be done by looking at the variable
importance of the random forest.

We performed grid search with oob (out-of-bag error) while tuning the
random forest to select the optimal values of the hyper-parameters try
(number of variables) and min.node.size (minimal node size):

```{r}
#getting optimal hyper parameters
mod_rf <- train(Y~., data = D_dtm, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))
mod_rf
```

The optimal values of the hyper-parameters are:

```{r}
mod_rf$bestTune
```

Now, we retrain the model with the selected hyperparameters to produce
the following output:

```{r}
rfGrid <- expand.grid(mtry = 43,
                      min.node.size = 5,
                      splitrule = "variance")
                      
mod_rf_tune <- train(Y~., data = D_dtm, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
```

Then, we split our data into train and test data sets:

```{r}
#splitting into train and test data
X <- D_dtm %>%
  select(-c(Y))
y <- D_dtm$Y

index <- createDataPartition(y, p=0.7, list=FALSE)
X_train <- X[index, ]
X_test <- X[-index, ]
Y_train <- y[index]
Y_test<- y[-index]

cat("Dimensions of the training set are", dim(X_train), "\n")
cat("Dimensions of the test set are", dim(X_test), "\n")
```

Next, we perform random forest on the train data set

```{r}
#random forest regression
regr <- randomForest(x = X_train, y = Y_train , maxnodes = 100 , ntree = 1000)
predictions <- predict(regr, X_test)

result <- X_test
result['Y'] <- Y_test
result['predictions'] <- predictions

head(result)
```

The graph below shows the data between Actual Average Salary vs.
Predicted Average Salary

```{r}
#plot graph to compare actual vs. predicted
ggplot(  ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Actual Average Salary", y = "Predicted Average Salary", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
```

```{r}
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , Y_test) ))
print(paste0('MSE: ' ,caret::postResample(predictions , Y_test)['RMSE']^2 ))
print(paste0('RMSE: ' ,caret::postResample(predictions , Y_test)['RMSE'])) #21.48
print(paste0('R2: ' ,caret::postResample(predictions , Y_test)['Rsquared'] ))
```

The bar graph below shows the top 5 most important variables in this
prediction

```{r}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)
var_importance
data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() + 
  xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
```

#### 3.1.1.2.1 XGBoost

First we split our data into test and training sets

```{r}
set.seed(100)

ind <- runif(nrow(D_dtm)) < 0.8

train_dtm <- D_dtm[ind , ]
test_dtm <- D_dtm[!ind , ]

cat("Dimensions of the training set are", dim(train_dtm), "\n")
cat("Dimensions of the test set are", dim(test_dtm), "\n")
```

Then we create our XGBoost model from caret, using hyperparameters as
shown below:

```{r}
library(caret)

set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 350,
  eta = 0.1,
  max_depth = 4,
  gamma = 0.02,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgbGrid_2 <- expand.grid(
  nrounds = seq(40, 100, length = 7),
  eta = seq(0.1, 0.6, length = 5),
  max_depth = seq(11, 20, length = 10),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y ~.,
                   data = train_dtm,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)

xgb_caret
```

```{r}
pred_y = predict(xgb_caret, test_dtm)

#measure prediction accuracy
caret::MAE(test_dtm$Y, pred_y) #mae
caret::RMSE(test_dtm$Y, pred_y) #rmse
```

```{r}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)

xgb_imp[1:20]

xgb.plot.importance(xgb_imp[1:20])
```

#### 3.1.1.3. Neural network

In order to predict the salary with neural network, we used Continuous
Bag of Words (CBOW) as follows. It enables us to do downstream tasks,
such as regression on texts.

```{r}
dtm <- create_dtm(it, vectorizer)
cbow_data <- as.matrix(dtm %*% word_vectors)
```

Here is the model:

```{r}
set.seed(8888)
all_data <- cbow_data %>%
  as_tibble %>%
  mutate(Y = D_clean$Avg.Salary.K.)

ind <- which(runif(nrow(all_data)) < 0.7)

train_data <- all_data %>% slice(ind)
test_data <- all_data %>% slice(-ind)

mod_nn <- neuralnet(Y ~ ., data = train_data, 
                                    hidden = c(20, 20),
                    threshold = 0.5,
                    lifesign = "full",
                    lifesign.step = 100,
                    stepmax = 10000,
                    err.fct = "sse",
                    linear.output = TRUE)

pred_nn <- compute(mod_nn,test_data[,1:100])
RMSE_nn <- (sum((test_data$Y - pred_nn$net.result)^2)/nrow(test_data))^0.5

RMSE_nn #41.20078
```

#### 3.1.1.4. Symbolic regression

#### 3.1.1.5. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a
modification of linear regression, as there is an extra regularization
term in its loss function as compared to that of linear regression. This
is to lower the complexity of the model by limiting the sum of the
absolute values of the coefficients. Meanwhile, elastic net regression
combines the properties of both linear and LASSO regressions, and there
are two hyperparameters, namely β and α. Here is the loss function of
elastic net regression: $$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$ We will need to choose the optimum values for these two
hyperparameters before generating predictions.

##Elastic Net

```{r}
elasticnet <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 5))

elasticnet
```

##LASSO

```{r}
lambda <- 10^seq(-3, 0, length = 20)
lambda
```

```{r}
lasso <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda), preProcess = c("scale"))

lasso
```

```{r}
lasso$bestTune
```

### 3.1.2. Beyond the syllabus

#### 3.1.2.1. Shapley Value Regression

Shapley Value Regression is a technique used to determine the relative
importance of predictor variables in linear regression. It derives from
game theory and its purpose is to evaluate the worth of each player's
input over all possible combinations of players. In the study, it aims
to identify main variables predicting data scientists' salaries and to
avoid the problem of high correlation between variables.

```{r}
set.seed(1234)
ind <- runif(nrow(D_clean)) < 0.7

train_data <- D_clean[ind , ] 
test_data <- D_clean[!ind , ]

avg_S = train_data$Avg.Salary.K
rating = train_data$Rating
age = train_data$Age
comp = train_data$comp_count

lmsalary = lm(avg_S ~ comp + age  + rating)
summary(lmsalary)

library(relaimpo)
calc.relimp(lmsalary, type=list('lmg'), rela=T)

library(ggplot2)
plot(lm(avg_S ~ comp + age  + rating))

#relative importance metrics
names <- c("comp", "age", "rating")
count = c(0.2792880, 0.5205512, 0.2001608)

barplot(count,names.arg=names,xlab="Explanatory Variables",ylab="Relative Importance of Variables")
```

From the results, Age of the company is the most important predictor
variable. It accounts for approximately 52.1% of the average salary.
Established companies are more stable and have funds to provide higher
salaries as compared to start-ups. In contrast, other variables such as
number of competitors and rating only account for 27.9% and 20% of the
average salary. "comp" might be slightly higher than rating because
companies have to offer more attractive salaries to employees so they
will be less likely to job-hop to their competitor companies. \####
3.1.2.2. Theory behind the model

## 3.2. Summary of results

# 4. Conclusion
