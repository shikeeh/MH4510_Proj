---
title: "MH4510 Project"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# MH4510 Project - Data science salary prediction

## Preface:

How to use Github:

1.  Open GitHub Desktop app
2.  Pull from origin to receive the most updated .Rmd file
3.  Make your edits on the MH4510_Project.Rmd file
4.  Save your edits in the same file
5.  Add descriptions for changes made
6.  Commit changes
7.  Push changes

Note that pulling the most update project file will override the project
file existing in your project folder. You may either create a fork to
save your changes, or simply create a separate .Rmd file to work on
personally before copying your edits to the main project file.

# 1. Introduction to the problem

## 1.1. Literature review

There is no doubt that COVID-19 has changed the employment landscape in
Singapore. A rapid shift towards digitalisation of businesses amidst a
recession means that Singaporeans have to keep pace with the change in
order to remain competitive at work (My Skills Future, 2021).

Singapore employers have prioritised skills over education (Tan, 2021).
Employers are starting to realise that new hires should be assessed
based on their existing skill sets --- instead of just their paper
qualifications and work history --- as there are various in-demand soft
skills transferable from one job to another. Other skills relevant to
the job may be learned through on-the-job training and development (The
Straits Times, 2021).

However, there is a gap between what individuals expect they will
require and what they have been taught in school among students. Just
27% of students say they are well equipped for future positions, while
22% say they are not at all prepared. According to a [2019
survey](https://www.cbi.org.uk/media/3841/12546_tess_2019.pdf) (Grimes,
2019), two out of five employers believe school and college graduates
are unprepared for employment. One-third say they are dissatisfied with
the quantity of relevant work experience young people have (EconoTimes,
2021).

The last decade has also shown us that whether or not one has a degree,
continual learning is imperative. In an age of ubiquitous disruption and
massive unpredictability, both employers and job seekers concede that
the knowledge and skills gained from a university degree can easily
become obsolete (Lim, 2021).

## 1.2. Objective

As the world grows more volatile, uncertain, complex and ambiguous,
graduates have to catch up with real-world needs and equip themselves
with the necessary skills. Considering that more employers are placing a
greater emphasis on individuals' skill sets and key traits than they are
on a piece of paper, this project aims to predict data scientist salary
based on the relevant variables in the dataset.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(neuralnet)
library(fastDummies) # for creating dummy variables

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
names(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) %>%
  mutate(Job.Title = tolower(Job.Title)) %>%
  mutate('desc_len' = sapply(D$Job.Description, nchar))

# Ssing stringr to replace all instances of "\n" with spaces instead
#while (grepl("\n", D_clean$cleaned_text, fixed = TRUE)) {
#  D_clean$cleaned_text <- str_replace(D_clean$cleaned_text, "\n", " ")
#}
# Used a while loop because the function would only run once through the string, removing only one instance of '\n'

head(D_clean)
```

2.  Re-doing columns to reflect simplified job titles and seniority:

```{r}
# Drop the versions that the owner created, so we can control how it is done
D_clean <- D_clean %>%
  select(-c('job_title_sim', 'seniority_by_title'))

# Create a function to collate general titles of job positions
job_simp_fn <- function(title) {
  if (grepl("data scientist", title)) {
  output <- "data scientist"
  } else if (grepl("director", title)) {
    output <- "director"
  } else if (grepl("manager", title)) {
    output <- "manager"
  } else if (grepl("data science", title)) {
    output <- "data scientist"
  } else if (grepl("data engineer", title)) {
    output <- "data engineer"
  } else if (grepl("data analyst", title)) {
    output <- "data analyst"
  } else if (grepl("data analytics", title)) {
    output <- "data analyst"
  } else if (grepl("machine learning", title)) {
    output <- "MLE"
  } else {
    output <- "NA"
  }}

# Create a function to collate seniority of job positions
seniority <- function(title) {
  if (grepl("sr", title)) {
  output <- "senior"
  } else if (grepl("senior", title)) {
    output <- "senior"
  } else if (grepl("jr", title)) {
    output <- "junior"
  } else if (grepl("junior", title)) {
    output <- "junior"
  } else {
    output <- 'NA'
  }}

# Apply both functions and create new variables 
D_clean$'job_simp' <- sapply(D_clean$Job.Title, job_simp_fn)
D_clean$'seniority' <- sapply(D_clean$Job.Title, seniority)

# View head of only these three variables:
head(D_clean[, c('Job.Title', 'job_simp', 'seniority')])
```

3.  Create a new variable to count the number of competitors a company
    has:

```{r}
# Function to convert "Competitors" variable from "-1" and strings to a count variable
my_strsplit <- function(string) {
  if (string == -1){
    output <- 0
  } else {
  output <- length(strsplit(toString(string), ", ")[[1]])
  }}

# Apply the function and create a new variable comp_count
D_clean$comp_count <- sapply(D_clean$Competitors, my_strsplit)

# View head of variables 'Competitors' and 'comp_count'
head(D_clean[, c('Competitors', 'comp_count')])
```

### 2.2.2. Data visualizations

Below is a summary of our numeric variables:

(this included categorical variables that used 1s and 0s, such as
python, employer.provided, etc. and we have yet to remove them).

```{r}
summary(D_clean%>%select_if(is.numeric))
```

#### 2.2.2.1. Numerical variables data visualizations

1.  Histograms

Histogram of rating:

```{r}
ggplot(D_clean, aes(x=Rating)) +
  geom_histogram()
```

-   Companies without rating are given a -1 rating. Since the rest of
    the ratings follow a normal distribution, we may replace the ratings
    with a mean value instead.

Histogram of average salary in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram()
```

Histogram of age of companies:

```{r}
ggplot(D_clean, aes(x=Age)) + 
  geom_histogram()
```

Histogram of job description text length:

```{r}
ggplot(D_clean, aes(x=desc_len)) +
  geom_histogram()
```

2.  Boxplots

Boxplot of salaries

```{r}
D_long <- melt(D_clean%>%select(c(Lower.Salary, Upper.Salary, Avg.Salary.K.)))
head(D_long)
ggplot(D_long, aes(x=variable, y=value)) +
  geom_boxplot()
```

Boxplot of Rating:

```{r}
ggplot(D_clean, aes(y=Rating)) +
  geom_boxplot()
```

Boxplot of Age:

```{r}
ggplot(D_clean, aes(y=Age)) +
  geom_boxplot()
```

Boxplot of Job Description Length:

```{r}
ggplot(D_clean, aes(y=desc_len)) + 
  geom_boxplot()
```

3.  Word cloud

a\. Generating our term-document matrix:

```{r}
corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
DTM <- DocumentTermMatrix(corpus)
dim(DTM)
```

-   (perhaps we can try GloVe instead)

Some frequent words, organized according to alphabetical order:

```{r}
words_freq <- termFreq(D_clean$cleaned_text)
head(words_freq)
```

b\. Some of these words do not make sense, so we increase the minimum
frequency of the word to 35 (We noticed at words_freq \>= 30 there were
still some hard to understand words, like abl = 33):

```{r}
frequent_words <- words_freq[words_freq >= 35]
length(frequent_words)
```

c\. The new dimensions of our term-document matrix with the frequent
words:

```{r}
DTM <- DTM[ , names(frequent_words)]
dim(DTM)
```

d\. Remove stopwords and plot word cloud. (Plus add some colours):

```{r}
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words), freq = frequent_words, min.freq = 0,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))
```

e\. Dimensions of term-document matrix after removing stopwords:

```{r}
DTM <- DTM[ , names(frequent_words)]
dim(DTM)
```

4.  Corrplot

```{r}
D_num <- D_clean %>%
  select(-c("index", "Python", "spark", "aws", "excel", "sql", "sas", "keras", "pytorch",  
            "scikit", "tensor", "hadoop", 'tableau', 'bi', 'flink', 'mongo', 'google_an', 
            "Employer.provided", 'Hourly')) %>% 
  rename('comp' = 'comp_count', 'desc' = 'desc_len', 'avg_S' = 'Avg.Salary.K.', 
         'upp_S' = 'Upper.Salary', 'low_S' = 'Lower.Salary')

D_num %>%
  ggcorr(palette = "RdBu", label = TRUE)
```

```{r}
D_num %>% 
  select_if(is.numeric) %>%
  ggpairs()
```

```{r}
D_num %>%
  select_if(is.numeric) %>%
  cor %>% round(3)
```

#### 2.2.2.2. Categorical variables data visualizations

Calling out names of variables in our dataset to find which are
categorical:

```{r}
names(D_clean)
```

Creating a new dataframe consisting of categorical variables:

```{r}
D_cat <- D_clean %>%
  select(c("Location", "Headquarters", "Size", "Type.of.ownership", "Industry", 
           "Sector", "Revenue", "company_txt", "Job.Location", 'Python', "spark", "aws", 
           "excel", "sql", "sas", "keras", "pytorch", "scikit", "tensor", "hadoop", 
           "tableau", "bi", "flink", "mongo", "google_an", "job_simp", "seniority"))
head(D_cat)
```

-   Note that variables like "Location", "Headquarters", "Industry", and
    "company_txt" have too many variables to properly plot the graphs

1.  Barplot of top 10 job locations (state):

```{r}
D_cat %>%
  count(Job.Location) %>%
  mutate(n_percent = prop.table(n)) %>%
  arrange(-n_percent) %>%
  slice(1:10) %>%
  ggplot(aes(x = reorder(Job.Location, -n_percent), y = n, fill = n, 
             label = scales::percent(n_percent))) + 
  geom_bar(position='dodge', stat="identity") + 
  geom_text(size=3, vjust = -0.5) + 
  geom_text(size=3, aes(label = n), vjust = 1.5, colour="white") +
  xlab("Job Location (State)") + ylab("N")
```

```{r}
ggplot(D_cat, aes(x=job_simp))
```

# 3. Objectives

## 3.1. Modelling

```{r}

```

```{r}

```

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

#### 3.1.1.2. Random forest/deep forest (gcforest)/XGBoost

#### 3.1.1.3. Neural network

```{r}
tokens <- space_tokenizer(D_clean$cleaned_text)
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it) %>% prune_vocabulary(10)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer)

cat("Dimensions of tcm =", dim(tcm), "\n")
```

```{r}
dim_word_emb <- 100

glove = GlobalVectors$new(rank = dim_word_emb, x_max = 10)

wv_main = glove$fit_transform(tcm, n_iter = 50, convergence_tol = 0.01)

wv_context = glove$components
word_vectors = wv_main + t(wv_context)

cat("Dim of word vector matrix =", dim(word_vectors))
```

```{r}
dtm <- create_dtm(it, vectorizer)
cbow_data <- as.matrix(dtm %*% word_vectors)
```

```{r}
set.seed(8888)
all_data <- cbow_data %>%
  as_tibble %>%
  mutate(Y = D_clean$Avg.Salary.K.)

ind <- which(runif(nrow(all_data)) < 0.7)

train_data <- all_data %>% slice(ind)
test_data <- all_data %>% slice(-ind)

mod_nn <- neuralnet(Y ~ ., data = train_data, 
                                    hidden = c(20, 20),
                    threshold = 1,
                    lifesign = "full",
                    lifesign.step = 10,
                    stepmax = 4000,
                    err.fct = "sse",
                    linear.output = TRUE)

pred_nn <- compute(mod_nn,test_data[,1:100])
pred_nn <- pred_nn$net.result*(max(all_data$Y)-min(all_data$Y))+min(all_data$Y)
test_n <- (test_data$Y)*(max(all_data$Y)-min(all_data$Y))+min(all_data$Y)
RMSE_nn <- (sum((test_n - pred_nn)^2)/nrow(test_data))^0.5

RMSE_nn
```

#### 3.1.1.4. Symbolic regression

#### 3.1.1.5. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a modification
of linear regression, as there is an extra regularization term in its loss
function as compared to that of linear regression. This is to lower the 
complexity of the model by limiting the sum of the absolute values of the 
coefficients. Meanwhile, elastic net regression combines the properties of 
both linear and LASSO regressions, and there are two hyperparameters, namely 
β and α. Here is the loss function of elastic net regression:
$$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$
We will need to choose the optimum values for these two hyperparameters 
before generating predictions.

##Elastic Net
```{r}
elasticnet <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 5))

elasticnet
```

##LASSO
```{r}
lambda <- 10^seq(-3, 0, length = 20)
lambda
```

```{r}
lasso <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda), preProcess = c("scale"))

lasso
```
```{r}
lasso$bestTune
```

### 3.1.2. Beyond the syllabus

#### 3.1.2.1. Proposed model

#### 3.1.2.2. Theory behind the model

## 3.2. Summary of results

# 4. Conclusion
