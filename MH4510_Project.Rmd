---
title: "**Studying the Impact of Job Descriptions on Data Science Salaries**"
subtitle: "Team Humilty"
date: "`r Sys.Date()`"
author: 
  - CHLOE NEO TZE CHING
  - ENG JING KEAT
  - GOH WEI LUN GLENN
  - KANEKO YOSHIKI
  - TAN WEI KEONG
output: 
  pdf_document:
    fig_caption: yes
    toc: true
    toc_depth: 3
    number_sections: true
    df_print: paged
abstract: "Millions of workers worldwide are no longer satisfied with just a fair paycheck, instead, they wish to achieve job satisfaction through meaningful career opportunities. This project aims to interpret the relationship between data science salaries and keywords in job descriptions. The data is cleaned by preparing job description text for analysis by converting it into a document-term matrix. The models used are Multiple Linear Regression (MLR), Random Forest (RF), XGBoost (XGB), Multivariate Adaptive Regression Splines (MARS), and Partial Least Squares (PLS), where their performances will be assessed in relation to one another. We found that MLR has the lowest Root Mean Square Error (RMSE) of 16.2. Thus MLR is the best performing model that accurately predicts salary, and we identified keywords used in this model."
editor_options: 
  markdown: 
    wrap: 72
---

\newpage

# Introduction to the problem

## Literature review

A rapid shift towards digitalisation of businesses has radically changed
the employment landscape in Singapore, which means Singaporeans need to
keep up with the changes if they wish to stay competitive at work (My
Skills Future, 2021). Employers in Singapore are starting to place an
emphasis on skills rather than education (Tan, 2021). New hires today
are assessed not just by their qualifications and work history, but also
by their soft skills as there are a variety of soft skills in demand
(The Straits Times, 2021).

According to a new report by Instant Offices, 73% of Singaporean workers
are dissatisfied with their jobs (Arora, 2022). When asked if they
planned to change jobs over the following six months, 31% of respondents
responded "yes" (Chong, 2022). Millions of workers worldwide are no
longer willing to return home with just a fair paycheck. They prefer to
know how well they are progressing towards a meaningful career, which is
wellness, freedom, security, and experience at work, so as to achieve
job satisfaction. As a result, job seekers should take all these factors
into consideration when applying for a job.

These factors can be broken down and identified as keywords in job
descriptions. Keywords are crucial to job adverts because they allow job
seekers to narrow their search related to a role, skill, or industry for
suitable employment (Alexander, 2019). Suitable individuals are more
likely to find the job post when employers and recruiters add key terms
and phrases that are relevant to a particular role. This increases the
percentage of successfully matching job seekers with their ideal jobs.

## Objective

This project aims to interpret the relationship between data science
salaries and keywords in job descriptions. Doing so will give insight to
keywords that have a significant impact on salaries. Job seekers will
also be able to set healthy salary expectations based on keywords that
reflect their skill set and values. Identified keywords can also give
insight to areas that job seekers can look to upskill in.

\newpage

# Data set

## Description of data set

[Click
here](https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor)
to access our data set. The owner scrapped the data from Glassdoor and
pre-processed it. It contains 41 variables, including the average data
science salary measured in thousand, and has 742 observations.

```{r, include = FALSE}
# Import relevant libraries:
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(knitr)
library(earth)
library(vip)
library(dplyr)
library(pls)
```

Below is a sample of our dataset:

```{r, echo = FALSE}
# import data from local csv file
D <- read_csv("data_cleaned_2021.csv")
knitr::kable(head(D[, 20:25]), 
             caption = "A sample of our dataset.")
```

\newpage

## Exploratory data analysis

### Feature selection

Since this project focuses on job description and its relationship to
salary, we will only keep the "Job Description" variable and the "Avg
Salary(K)" variable. In the next section, we process our "Job
Description" variable so that it is usable for our models.

### Data cleaning

We clean our "Job Description" variable by creating a new variable
"cleaned_text" that stores only alphanumeric characters and converts all
the text to lowercase. We then only keep the "Avg Salary(K)" and
"cleaned_text" variables.

```{r, echo = FALSE}
# Keep only alphabets and spaces, changing texts to lowercase, and remove all "\n"
D_clean <- D %>%
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", D$`Job Description`)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) 
```

```{r, echo = FALSE}
#remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(`Avg Salary(K)`, cleaned_text))
```

### Data visualizations

To get a better understanding of the data we are working with, we
perform some data visualizations. First, we take a look at the
distribution of the average salary (in thousands).

```{r, echo = FALSE, fig.cap="\\label{fig:figs}Histogram of salaries"}
ggplot(D_clean, aes(x=`Avg Salary(K)`)) + 
  geom_histogram(fill = "darkgreen", alpha = 0.65, bins = 20)
```

From our histogram, we see that our average salary (in thousands)
roughly follows a normal distribution. Hence we will predict the raw
annual salary without further transformations.

Secondly, we visualize the job descriptions with a word cloud after
removing words that occur less than 40 times, and stopwords such as
"the", "he" etc. as these are common words that do not store any
informational value in our analysis. Below we show 100 of the most
common words.

```{r, echo = FALSE, warning = FALSE, fig.cap="\\label{fig:figs}Job description word cloud"}
visualize_text <- function(x) {
  # x is a character vector
  # the function will extract
  frequent_words <- termFreq(x)
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 40,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

### Document-term matrix

The document-term matrix (DTM) is a table reflecting the frequency of
each word where the column names represent words and the row names
represent documents, i.e., the reviews.

We create a DTM for our job descriptions, setting minimum word frequency
to 40.

```{r, echo = FALSE, comment = ""}
corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]

cat("Setting minimum word frequency to 40, we retain", length(frequent_words), "words out of the original", length(words_freq), "words.", "\n")
```

```{r, echo = FALSE, comment = ""}
dtm <- dtm[ , names(frequent_words)]

cat("Our document-term matrix consists of", nrow(dtm), "job descriptions against", ncol(dtm), 'words present in our vocab after removing stopwords.', '\n')
```

We create a new data frame for our DTM and produce a sample of it below.

```{r, echo = FALSE}
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y_salary = D_clean$`Avg Salary(K)`) %>%
  select(c(Y_salary, everything()))

knitr::kable(head(D_dtm[, c('data', 'machine', 'python', 'analytics', 'science')]), 
             caption = "A sample of our DTM.")
```

\newpage

# Modelling

## Feature selection using LASSO

We will use LASSO regularization to select features to prepare our data
for three of our models: Multiple LInear Regression, Random Forest, and
XGBoost. The last two models use their own methods of feature selection,
so we will not use features selected by LASSO for those two models, but
the entire dataset instead.

The LASSO procedure is as follows:

1.  Use cross-validation to find the best value of lambda (approx.
    0.359)
2.  Store our variables selected by LASSO
3.  Prepare our final dataset to be used for MLR, RF, and XGB

```{r, include = FALSE}
# we remove all output for the actual report.
# to show the output of our model and the lambda coefficient plot, remove 
# "include = FALSE" written above

set.seed(100)

lambda <- 10^seq(-1, 0 , length = 10)

lasso <- train(
  Y_salary ~., data = D_dtm, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso

lambda_for_plotting <- 10^seq(from = -1, to = 1.2, length = 100)

lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
  as.matrix %>% t %>% as_tibble %>%
  mutate(lambda = lambda_for_plotting)

lasso_coefs %>%
  pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
  ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
  geom_line() + scale_x_log10() + theme(legend.position = "none")

store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>% 
  select_if(function(col) !all(col == 0))

selected_var <- names(store[2:(ncol(store)-1)])

D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
  mutate(Y_salary = D_dtm$Y_salary) %>%
  select(c(Y_salary, everything()))
```

```{r}
knitr::kable(head(D_final[, c('Y_salary', 'predictive', 'education', 'learn', 
                              'machine', 'experience', 'analytics', 'support')]), 
             caption = "A sample of our final dataset.")
```

We also prepare our training and test data to be used, and the
dimensions are as follows:

```{r, echo = FALSE, comment = ""}
set.seed(100)
ind <- runif(nrow(D_final)) < 0.8

train_data <- D_final[ind , ]
test_data <- D_final[!ind , ]

cat("Dimensions of the training set are", dim(train_data), "\n")
cat("Dimensions of the test set are", dim(test_data), "\n")
```

```{r, echo = FALSE, comment = ""}
# below is our training and test data set without Y_salary

train_data_nolabel <- train_data %>%
  select(-c(Y_salary))
test_data_nolabel <- test_data %>%
  select(-c(Y_salary))
```

From this, we can see that what started with over 10,000 unique words in
our job descriptions has been narrowed down to 354 words.

\newpage

## Models

### Multiple Linear Regression

The first model we use is Multiple Linear Regression, using the 354
features selected by LASSO.

```{r, echo = FALSE, comment = "", warning = FALSE}
mlr_mod <- lm(Y_salary ~ .,
               data = train_data)

mlr_mod %>%
  predict(test_data) %>%
  caret::RMSE(test_data$Y_salary)
```

Our MLR model's RMSE value is 16.20561, the lowest we will achieve in
this project.

Below is a bar plot of the top 20 variables with the highest absolute
coefficients. The sign of their coefficients reflect a positive or
negative relationship with the predicted salary.

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}Top 20 variables according to absolute coefficients"}
var_coeff <- data.frame(coefficients(mlr_mod)) %>%
  arrange(desc(abs(coefficients(mlr_mod)))) %>%
  dplyr::slice(2:21)

ggplot(var_coeff, aes(x=coefficients.mlr_mod., 
                      y=reorder(rownames(var_coeff), +coefficients.mlr_mod.))) +
  geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
  xlab("Coefficients") + ylab("Key words")
```

This plot shows that the presence of words such as "eligible" and
"molecules" in the job description will result in a higher predicted
salary, whereas words such as "streaming" and "wellness" in the job
description will lower the predicted salary.

\newpage

### Random Forest

The next step is to visualize our data using random forest decision
trees. We first train the data to obtain its optimal hyper-parameters.
We then perform grid search on the optimal hyper-parameter values to
minimize the out-of-bag error.

```{r, include = FALSE}
# again, remove "include = FALSE" to show the model produced.

set.seed(100)

#getting optimal hyper parameters
mod_rf <- train(Y_salary ~., data = D_final, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))

mod_rf
```

The optimal values of the hyper-parameters are:

```{r, echo = FALSE, comment = "", warning = FALSE}
knitr::kable(mod_rf$bestTune)
```

So we retrain the model with the selected hyper-parameters, and fit our
Random Forest model on the training data set.

```{r, include = FALSE}
# again, remove "include = FALSE" to see the output of the model

set.seed(100)

rfGrid <- expand.grid(mtry = 178,
                      min.node.size = 5,
                      splitrule = "extratrees")
                      
mod_rf_tune <- train(Y_salary ~., data = D_final, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))

mod_rf_tune
```

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

regr <- randomForest(x = train_data_nolabel, 
                     y = train_data$Y_salary, 
                     maxnodes = 100 , 
                     ntree = 1000)

predictions_rf <- predict(regr, test_data_nolabel)

result <- test_data
result['predictions'] <- predictions_rf

knitr::kable(head(result[, c('predictions', 'Y_salary', 'education', 'learn', 
                              'machine', 'experience', 'analytics', 'support')]), 
             caption = "A sample of our Random Forest prediction results.")
```

Our RMSE for the Random Forest model is shown below:

```{r, echo = FALSE, comment = "", warning = FALSE}
#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, predictions_rf), 2), "\n")
```

The bar graph below shows the top 20 most important variables in this
prediction. Based on the bar graph, the top 5 most important variables
are "machine", "analyst", "give", "phd", and "scientists".

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}Random forest top 20 most important variables"}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)

data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = importance, y = reorder(variable, importance))) + 
  geom_col(fill = "darkgreen", alpha = 0.7) + 
  xlab("Importance") + ylab("Variables") 
```

\newpage

### XGBoost

Like random forests, gradient boosting machines does classification
based on decision trees. However, while random forest builds an ensemble
of deep (i.e complex) trees that are independent of one another,
gradient boosting machines build shallow trees sequentially, where each
tree learns and improves from the previous tree. This means that one
would start with a weak model and sequentially boost its performance by
allowing each new tree to focus on training data where the previous tree
had the largest errors in prediction (or residuals). This is done by
fitting each tree in the sequence according to the residuals of the
previous tree.

Moreover, it computes the second-order gradients, i.e. second partial
derivatives of the loss function, which provides more information about
the direction of gradients and how to get to the minimum of our loss
function while gradient boost uses the loss function of simple decision
tree model as a proxy to minimize the error of the overall model. In
addition, it uses advanced regularization (L1 and L2), which improves
model generalization.

Each weight in all the trees would be multiplied by the learning rate in
an XGBoost model, such that

$$
w_j = \text{Learning Rate} \times \frac{\sum_{i\in I_j}\frac{\partial Loss}{\partial(\hat y=0)}}{\sum_{i\in I_j}\frac{\partial^2 Loss}{\partial(\hat y=0)^2} + \lambda}
$$

where $I_j$ is a set containing all the instances (($x, y$) data points)
at a leaf, and $w_j$ is the weight at leaf $j$ with regularization from
the $\lambda$ constant.

We create our XGBoost model from the caret library, using
hyperparameters as shown below:

```{r, include = FALSE}
set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 650,
  eta = 0.21,
  max_depth = 3,
  gamma = 0.04,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y_salary ~.,
                   data = train_data,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)
```

As shown above, we have used these hyperparameters:

1.  *gamma*: Pseudo-regularisation hyperparameter that controls the
    complexity of each tree.

2.  *nrounds*: Number of decision trees in the final model

3.  *eta*: Learning rate; determines the contribution of each tree on
    the final outcome and also how quickly the algorithm goes down the
    gradient descent.

4.  *max_depth*: Depth of each tree

5.  *min_child_weight*: Minimum number of observations in terminal
    nodes; controls complexity of the trees

6.  *colsample_bytree*: subsample of columns used for each tree
    (repeated for every tree)

7.  *subsample*: subsampling ratio of training data for growing trees to
    prevent over-fitting

The hyperparameters were tuned using 5-fold cross validation and grid
search to find the best model, and we arrived at the optimal values for
the hyperparameters.

Below we compute our RMSE.

```{r, echo = FALSE, comment = "", warning = FALSE}
pred_y_xgb = predict(xgb_caret, test_data)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, pred_y_xgb), 2), "\n")
```

Overall, XGBoost gave an RMSE value of 16.59.

Below we extracted the 20 most important features (words) from the
XGBoost model.

```{r, echo = FALSE, comment = "", warning = FALSE}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)

knitr::kable(head(xgb_imp), 
             caption = "Some of our 20 most important features.")
```

Plotting our top 20 most important variables:

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}XGBoost top 20 most important variables"}
ggplot(xgb_imp[1:20], aes(x = Gain, y = reorder(Feature, Gain))) + 
  geom_col(fill = "darkgreen", alpha = 0.7) + 
  xlab("Importance") + ylab("Variables") 
```

\newpage

### Multivariate Adaptive Regression Splines (MARS)

We used multivariate adaptive regression splines (MARS) (Friedman 1991)
model here, it is an approach that automatically generates a piecewise
linear model that serves as an understandable stepping stone into
non-linearity after learning the notion of multiple linear regression.

By evaluating cutpoints (knots) similar to step functions, MARS offers a
practical method to capture the nonlinear relationships in the data.
This method evaluates every data point for every predictor as a knot and
builds a linear regression model using the candidate feature(s).

Consider non-linear, non-monotonic data where $Y = f(X)$. The MARS
method will initially search for a single point within a range of $X$
values where two distinct linear relationships between $Y$ and $X$
provide the lowest loss. The outcome is referred to as a hinge function
$h(x-a)$, where $a$ is the cutpoint value.

For example, if $a = 1$, our hinge function is $h(x-1)$ such that the
linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1 
\\ \beta_0 + \beta_1(x-1) \, ,& x > 1 \\ \end{cases}$$ After the first
knot is identified, the search for a second one begins, and it is
discovered at $x=2$. Now the linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1
\\ \beta_0 + \beta_1(x-1) \, ,& 1 < x < 2 
\\ \beta_0 + \beta_1(2-x) \, ,& x > 2 \\ \end{cases}$$ This process is
repeated until several knots are identified, leading to the creation of
a highly non-linear prediction equation. Even if using a lot of knots
could help us fit a particularly excellent relationship to our training
data, it might not perform well to unseen data. Once all of the knots
have been found, we may systematically eliminate knots that do not
significantly improve predictive accuracy. This is pruning process, and
we may use cross-validation to determine the optimal number of knots.

We will use the following packages. First of all, we divided the dataset
into training dataset and test dataset:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

ind_mars <- which(runif(nrow(D_dtm)) < 0.7)

train_mars <- D_dtm %>% dplyr::slice(ind_mars)
test_mars <- D_dtm %>% dplyr::slice(-ind_mars)

cat("Dimensions of the MARS training dataset are", dim(train_mars), "\n")
cat("Dimensions of the MARS test dataset are", dim(test_mars), "\n")
```

MARS model have two hyperparameters: the maximum degree of interactions
and the number of terms retained in the final model. To achieve the
optimal combination of these tuning parameters, we must conduct a grid
search that minimize the error of prediction.

Here, we built up a grid with 30 different combinations of interaction
complexity (degree) and the number of terms to include in the final
model (nprune).

```{r, include = FALSE}
marsgrid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 100, length.out = 20) %>% floor()
)
```

We performed required grid search by using 10-fold cross-validation:

-   Our chosen parameters.

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

mars_cv <- train(Y_salary ~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid
)

knitr::kable(mars_cv$bestTune) #nprune = 11, degree = 1
```

```{r, echo = FALSE, comment = "", warning = FALSE}
knitr::kable(mars_cv$results %>%
  filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree))
```

The backwards elimination feature selection process used in MARS models
seeks for reductions in the generalized cross-validation (GCV) estimate
of error when each additional predictor is introduced to the model. The
variable importance is based on this overall reduction. MARS effectively
accomplishes automated feature selection since it will automatically
include and remove variables throughout the pruning phase.

After pruning, a predictor's significance value is 0 if it was never
used in any of the MARS basis functions in the final model. There are
only 9 features have importance values greater than 0, whereas the other
features all have importance values of zero since they were excluded
from the final model.

We also kept track of how the residual sums of squares (RSS) change when
terms are added. However, we noticed that there is no much difference
between these two measures.

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}MARS top 20 most important variables"}
plot_GCV <- vip(mars_cv, num_features = 20, geom = "col", value = "gcv", 
                aesthetics = list(fill = "darkgreen", alpha = 0.7)) + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "col", value = "rss", 
                aesthetics = list(fill = "darkgreen", alpha = 0.7)) + ggtitle("RSS")

gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
```

We used the optimal hyperparameters to train the model and then
calculated the RMSE:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

marsgrid_tuned <- expand.grid(
  degree = 1, 
  nprune = 11
)

mars_cv_tuned <- train(Y_salary ~., data = train_data,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid_tuned
)

pred_mars = predict(mars_cv_tuned, test_data)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_data$Y_salary, pred_mars), 2), "\n")
```

\newpage

### Partial Least Squares (PLS)

Partial Least Squares (PLS) is a common technique to analyse relative
importance when the data includes more predictors than observations. It
is an useful dimension reduction method which is similar with principal
component analysis (PCA).

We do a regression against the response variable inside the narrower
space created by mapping the predictor variables to a smaller set of
variables. The response variable is not taken into account during the
dimension reduction process in PCA. PLS, on the other hand, seeks to
select newly mapped factors that best describe the response variable.

Below are the required packages. We divided the dataset into training
dataset and test dataset first:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)
# 70/30 split
inTraining <- createDataPartition(D_dtm$Y_salary, p = .7, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]
```

The hyperparameter for PLS model is the number of components used in the
model (ncomp) .We conduct a grid search that minimize the prediction
error to achieve the optimal hyperparameter. The grid search was
conducted by 10-fold cross-validation:

```{r, include = FALSE}
set.seed(100)

plsGrid <- expand.grid(
  ncomp   = seq(1, 100, by = 1)
)

pls_cv <- train(
  Y_salary ~ .,
  data = train_pls,
  method = 'pls',
  metric = "RMSE",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid
)

pls_cv
```

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}Number of components in PLS model vs. RMSE"}
plot(pls_cv)

head(pls_cv$results)

ggplot(pls_cv$results, aes(x = ncomp, y = RMSE)) +
  geom_point(colour = "darkgreen", alpha = 0.7) +
  labs(x = "No. of Components", y = "RMSE (Cross-Validation)")
```

We used the optimal hyperparameter to train the model and calculated the
RMSE as well:

```{r, echo = FALSE, comment = "", warning = FALSE}
set.seed(100)

plsGrid_tuned <- expand.grid(
  ncomp = 9
)

pls_cv_tuned <- train(Y_salary ~., data = train_pls,
  method = "pls",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid_tuned
)

pred_pls = predict(pls_cv_tuned, test_pls)

#measure prediction accuracy
cat("RMSE:", round(caret::RMSE(test_pls$Y_salary, pred_pls), 2), "\n")
```

The barplots below show that 'addition', 'generate', 'hypotheses',
'framework' and 'characteristics' are positive predictors, while
'streaming', 'francisco', 'difference', 'mongodb' are negative
predictors:

```{r, echo = FALSE, comment = "", warning = FALSE, fig.cap="\\label{fig:figs}PLS top 20 most important variables"}
coefficients_pls <- coef(pls_cv_tuned$finalModel)
sum.coef <- sum(sapply(coefficients_pls, abs))
coefficients_pls <- coefficients_pls * 100 / sum.coef
coefficients_pls <- sort(coefficients_pls[, 1 , 1])

coefficients_pls_df <- data.frame(coefficients_pls) %>%
  arrange(desc(abs(coefficients_pls))) %>%
  dplyr::slice(1:20)

ggplot(coefficients_pls_df, aes(x=coefficients_pls, 
                      y=reorder(rownames(var_coeff), coefficients_pls))) +
  geom_bar(stat = "identity", fill="darkgreen", alpha = 0.65) +
  xlab("Coefficients") + ylab("Key words")
```

\newpage

## Summary of results

```{r, echo = FALSE, comment = "", warning = FALSE}
final_results <- data.frame(Model = c('MLR', 'Random Forest', 'XGBoost', 'MARS', 'PLS'),
                            RMSE = c(16.21, 21.59, 16.59, 30.00, 23.88))
knitr::kable(final_results, 
             caption = "Accuracy of models.")
```

# Conclusion

Recommendation engines are a commonly found solution applied to job
search portals, such as the Singapore government's national jobs portal,
MyCareersFuture. However, more can be done to bridge the gap between job
seekers and their desired careers. In this project, we have produced
models that predict the expected average salary earned given a job
description. Our best performing model, Multiple Linear Regression, can
be used to help workers set expectations of salary based on keywords
they value as important in search of a job. This will help job seekers
focus on searching for their desired job role rather than focus on
maximizing salary earned, which will hopefully increase job
satisfaction. The model also identifies key terms such as "eligible",
"mongodb", "upon", "francisco", and "hypotheses". Some of these keywords
may not seem to make sense, and understanding the importance of these
words is unclear. Some of these words, on the other hand, give insight
into areas that job seekers can focus on, be it upskilling (for example,
learning MongoDB), or narrowing their job search to sectors such as
actuarial science or molecular chemistry in order to maximize their
potential salary.

Based on RMSE, our best model uses Multiple Linear Regression, and it
has identified key terms that have a significant impact on data science
salary. However, Multiple Linear Regression does not identify the same
important features (words) as our other models. Further research is
required to better understand the difference between models and why they
identify vastly different features as important.

\newpage

# References

1.  Alexander, L. (2019). The importance of keywords in job ads. SEEK.
    Retrieved November 16, 2022, from
    <https://www.seek.com.au/employer/hiring-advice/the-importance-of-keywords-in-job-ads>
2.  Arora, P. (2022, August 29). What's keeping Singapore employees
    unhappy at work? - ETHRWorldSEA. HR News Southeast Asia. Retrieved
    November 7, 2022, from
    <https://hrsea.economictimes.indiatimes.com/news/employee-experience/whats-keeping-singapore-employees-unhappy-at-work/93833788>
3.  Chong, C. (2022, May 17). Nearly 1 in 3 workers in S'pore plans to
    change employers in first half of 2022: Survey. The Straits Times.
    Retrieved November 4, 2022, from
    <https://www.straitstimes.com/singapore/jobs/nearly-1-in-3-workers-in-spore-plan-to-change-employers-in-first-half-of-2022-survey>
4.  My Skills Future. (2021, June 21). 5 Crucial Skills You Need to
    Remain Employable in the Wake of Covid-19 \| Myskillsfuture.gov.sg.
    MySkillsFuture. Retrieved October 14, 2022, from
    <https://www.myskillsfuture.gov.sg/content/portal/en/career-resources/career-resources/education-career-personal-development/5-crucial-skills-you-need-to-remain-employable-during-covid.html>
5.  The Straits Times. (2021, December 22). It's a match: How
    skills-based hiring fits in the future of work. The Straits Times.
    Retrieved October 14, 2022, from
    <https://www.straitstimes.com/singapore/jobs/its-a-match-how-skills-based-hiring-fits-in-the-future-of-work>
6.  Tan, E. (2021, April 14). S'pore employers prioritise skills over
    education, experience: LinkedIn survey. The Straits Times. Retrieved
    October 14, 2022, from
    <https://www.straitstimes.com/singapore/jobs/singapore-employers-prioritise-skills-over-education-experience-linkedin-survey>

\
\
