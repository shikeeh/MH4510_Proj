---
title: "Data Science Salary Prediction"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# Team Humility

Chloe Neo Tze Ching Eng Jing Keat Goh Wei Lun Glenn Kaneko Yoshiki Tan
Wei Keong

## Abstract

Data mining is the process of sorting through large data sets to
identify patterns and relationships, where we aim to gain meaningful
insights through the use of algorithms to predict future trends in
textual data. In this project, we aim to uncover key insights in data
science salary by developing a model to effectively associate keywords
in job descriptions to high paying analytics roles. The data is cleaned
by filtering the keywords in job descriptions with a minimum occurrence
of 40. Least Absolute Shrinkage and Selection Operator (LASSO) is then
applied to select the most important variables to be used in the model.
The models used are Multiple Linear Regression (MLR), Partial Least
Squares (PLS), Random Forest, XGBoost and Multivariate Adaptive
Regression Splines (MARS), where their performances will be assess in
relation to one another. Across all models, XGBoost has the lowest Root
Mean Square Error (RMSE) of (insert value). In other words, XGBoost is
the best model that accurately predicts salary. Based on XGBoost
results, some of the most important keywords in job descriptions towards
getting a higher salary in a Data Scientist Role are \"machine\",
\"phd\", \"education\", \"analyst\", and \"infrastructure\".

# 1. Introduction to the problem

## 1.1. Literature review

A rapid shift towards digitalisation of businesses has radically changed
the employment landscape in Singapore, which means Singaporeans need to
keep up with the changes if they wish to stay competitive at work (My
Skills Future, 2021). Employers in Singapore are starting to place an
emphasis on skills rather than education (Tan, 2021). New hires today
are assessed not just by their qualifications and work history, but also
by their soft skills as there are a variety of soft skills in demand
(The Straits Times, 2021).

According to a new report by Instant Offices, 73% of Singaporean workers
are dissatisfied with their jobs. When asked if they planned to change
jobs over the following six months, 31% of respondents responded "yes".
Millions of workers worldwide are no longer willing to return home with
just a fair paycheck. They prefer to know how well they are progressing
towards a meaningful career, which is a wellness, freedom, security, and
experience at work, so as to achieve job satisfaction. As a result, job
seekers should take all these factors into consideration when applying
for a job.

These factors can be further broken down into keywords in job
descriptions. Keywords are crucial to job advertisements because they
allow job seekers to narrow their search related to a role, skill, or
industry for suitable employment. Ideal individuals are more likely to
find the job post when hirers and recruiters add key terms and phrases
that are relevant to a particular role. This increases the percentage of
successfully matching job seekers with their ideal jobs.

## 1.2. Objective

As most employers will be impressed when they notice that resume is
customised, highlighting relevant skills and using certain keywords
suited to that particular company or position, job seekers can use these
findings to narrow down their job search based on their own preferences
such as salary range or skills set. As such, this project aims to
predict data science salary based on the keywords in job descriptions.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  # create a new variable containing only lowercase text from Job.Description
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) 

head(D_clean)
```

### 2.2.2. Feature selection

Our project's focus is on job descriptions and their relationship with
data science salaries. As such, we will only keep these two variables

```{r}
#remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(Avg.Salary.K., cleaned_text))
head(D_clean)
```

### 2.2.3. Data visualizations

1.  Histograms of average salaries in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram(fill = "blue", alpha = 0.65, bins = 20)
```

2.  Boxplots of average salaries:

```{r}
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
  geom_boxplot(fill = "blue", alpha = 0.65)
```

3.  Word cloud for job descriptions:

```{r}
visualize_text <- function(x) {
  # x is a character vector
  # the function will extract
  frequent_words <- termFreq(x)
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 40,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

We set the minimum occurrence of words to 40, and show the 100 most
common words above.

### 2.2.4. Feature engineering

Next we create a document-term matrix for our job descriptions, setting
minimum word frequency to 40

```{r}
library(tm)
library(wordcloud)

corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]

cat("Setting minimum word frequency to 40, we retain", length(frequent_words), "words out of the original", length(words_freq), "words.", "\n")
```

```{r}
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
dtm <- dtm[ , names(frequent_words)]

cat("Our document-term matrix consists of", nrow(dtm), "job descriptions against", ncol(dtm), 'words present in our vocab after removing stopwords.', '\n')
```

Next, we create a new dataframe for our DTM

```{r}
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y = D_clean$Avg.Salary.K.)

head(D_dtm)
```

# 3. Objectives

## 3.1. Modelling

LASSO

We will try the following values of lambda for our LASSO regularization:

```{r}
lambda <- 10^seq(-1, 0 , length = 10)
lambda
```

Below we train our LASSO:

```{r}
set.seed(100)

lasso <- train(
  Y ~., data = D_dtm, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso
```

Let us plot coefficients of LASSO

```{r}
lambda_for_plotting <- 10^seq(from = -1, to = 1.5, length = 100)
lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
  as.matrix %>% t %>% as_tibble %>%
  mutate(lambda = lambda_for_plotting)
head(lasso_coefs)

lasso_coefs %>%
  pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
  ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
  geom_line() + scale_x_log10() + theme(legend.position = "none")
```

Now we store our variables selected by LASSO

```{r}
store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>% 
  select_if(function(col) !all(col == 0))

selected_var <- names(store2[2:(ncol(store)-1)])

head(selected_var)
```

Finally we prepare our final dataset to be used for our models

```{r}
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
  mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
  select(c(Y_salary, everything()))

head(D_final)
```

Now we can prepare our training and test data:

```{r}
# Libraries used:
library(tidyverse)
library(stargazer)
library(caret)

set.seed(100)
ind <- runif(nrow(D_final)) < 0.8

train_data <- D_final[ind , ]
test_data <- D_final[!ind , ]

cat("Dimensions of the training set are", dim(train_data), "\n")
cat("Dimensions of the test set are", dim(test_data), "\n")
```

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

```{r}
k <- 5

set.seed(100)

add_folds_to_data <- function(dataset, k) {
  N <- nrow(dataset)
  folds <- ceiling((1:N)/(N/k))
  dataset %>% 
    mutate(fold = sample(folds, N))
}

train_mlr <- train_data %>%
  add_folds_to_data(k)

head(train_mlr)
```

Then we check to see if our data was properly split

```{r}
table(train_mlr$fold)
```

```{r}
var_list <- paste(names(train_mlr[2:355]), collapse = ", ")
var_list

mlr_mod <- lm(Y_salary ~ polym(var_list, degree = 1, raw = TRUE),
               data = train_mlr)

stargazer(mlr_mod, type = 'text')
```

We build functions to train models of different degrees and to calculate
RMSE

```{r}
train_poly_model <- function(deg, dataset = train_mlr) {
  mlr_mod <- lm(Y_salary ~ polym(names(train_mlr[2:355]), degree = deg, raw = TRUE),
               data = dataset)
}

error_rate <- function(mlr_mod, dataset = test_data) {
  mlr_mod %>%
    predict(dataset) %>%
    rmse(dataset$Y_salary)
}

train_and_validate <- function(deg, train = train_mlr, test = test_data) {
  train_poly_model(deg, train) %>%
    error_rate(test)
}
```

Create a function to apply different degrees to our previous functions,
and compile them altogether to create a data frame consisting of CV
error and test error values

```{r}
cv_and_test_error <- function(d, K = k) {
  # K is the number of folds, the default value is whatever is specified above
  # when we assigned k
  cv_error <- 1:K %>% sapply(
    function(i) train_and_validate(d, train_mlr %>% filter(fold != i),
                                 train_mlr %>% filter(fold == i))
    ) %>% mean
  
  test_error <- train_and_validate(d, train_mlr, test_data)
  
  c(deg = d, CV = cv_error, TEST = test_error)
}

cv_and_test_error(1)

all_degrees <- 1:5
final_errors <- sapply(all_degrees, cv_and_test_error) %>% t %>%
  as.data.frame
```

```{r}
final_errors
```

We do only up to degree 5 because on further testing, greater degrees
resulted in gross increases in CG and test errors, which indicates gross
overfitting. Cross validation has shown that a degree 1 polynomial works
best.

We should perhaps perform regularization first to select continuous
variables via cross-validation, and then apply cross-validation again to
select the most appropriate degree.

#### 3.1.1.2. Random forest

Next, we want to visualise our data through the use of decision trees.
However, decision trees alone have a high variance which can cause the
trees trained on different parts of the dataset to look very different.
Hence, we will introduce an ensemble method to reduce the variance.
Random forest is chosen over a bagged ensemble as there may exist one or
more very strong predictors in our dataset, causing other predictors to
not have a chance to be included. This will cause the predictions of
trees to be highly correlated and adding more trees will not reduce the
variance. Variable selection can also be done by looking at the variable
importance of the random forest.

We performed grid search with oob (out-of-bag error) while tuning the
random forest to select the optimal values of the hyper-parameters try
(number of variables) and min.node.size (minimal node size):

```{r}
#getting optimal hyper parameters
mod_rf <- train(Y~., data = D_dtm, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))
mod_rf
```

The optimal values of the hyper-parameters are:

```{r}
mod_rf$bestTune
```

Now, we retrain the model with the selected hyperparameters to produce
the following output:

```{r}
rfGrid <- expand.grid(mtry = 43,
                      min.node.size = 5,
                      splitrule = "variance")
                      
mod_rf_tune <- train(Y~., data = D_dtm, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
```

Then, we split our data into train and test data sets:

```{r}
#splitting into train and test data
X <- D_dtm %>%
  select(-c(Y))
y <- D_dtm$Y

index <- createDataPartition(y, p=0.7, list=FALSE)
X_train <- X[index, ]
X_test <- X[-index, ]
Y_train <- y[index]
Y_test<- y[-index]

cat("Dimensions of the training set are", dim(X_train), "\n")
cat("Dimensions of the test set are", dim(X_test), "\n")
```

Next, we perform random forest on the train data set

```{r}
#random forest regression
regr <- randomForest(x = X_train, y = Y_train , maxnodes = 100 , ntree = 1000)
predictions <- predict(regr, X_test)

result <- X_test
result['Y'] <- Y_test
result['predictions'] <- predictions

head(result)
```

The graph below shows the data between Actual Average Salary vs.
Predicted Average Salary

```{r}
#plot graph to compare actual vs. predicted
ggplot(  ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = Y_test, y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Actual Average Salary", y = "Predicted Average Salary", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
```

```{r}
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , Y_test) ))
print(paste0('RMSE: ' ,caret::postResample(predictions , Y_test)['RMSE'])) #21.48
```

The bar graph below shows the top 20 most important variables in this
prediction. Based on the bar graph, the top 5 most important variables
are ..

```{r}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)
var_importance
data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() + 
  xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
```

#### 3.1.1.3. XGBoost

First we split our data into test and training sets

```{r}
set.seed(100)

ind <- runif(nrow(D_dtm)) < 0.8

train_dtm <- D_dtm[ind , ]
test_dtm <- D_dtm[!ind , ]

cat("Dimensions of the training set are", dim(train_dtm), "\n")
cat("Dimensions of the test set are", dim(test_dtm), "\n")
```

Then we create our XGBoost model from caret, using hyperparameters as
shown below:

```{r}
library(caret)

set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 350,
  eta = 0.1,
  max_depth = 4,
  gamma = 0.02,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgbGrid_2 <- expand.grid(
  nrounds = seq(40, 100, length = 7),
  eta = seq(0.1, 0.6, length = 5),
  max_depth = seq(11, 20, length = 10),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y ~.,
                   data = train_dtm,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)

xgb_caret
```

```{r}
pred_y = predict(xgb_caret, test_dtm)

#measure prediction accuracy
caret::MAE(test_dtm$Y, pred_y) #mae
caret::RMSE(test_dtm$Y, pred_y) #rmse
```

```{r}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)

xgb_imp[1:20]

xgb.plot.importance(xgb_imp[1:20])
```

#### 3.1.1.4. Multivariate Adaptive Regression Splines (MARS)

We used multivariate adaptive regression splines (MARS) (Friedman 1991)
model here, it is an approach that automatically generates a piecewise
linear model that serves as an understandable stepping stone into
non-linearity after learning the notion of multiple linear regression.

By evaluating cutpoints (knots) similar to step functions, MARS offers a
practical method to capture the nonlinear relationships in the data.
This method evaluates every data point for every predictor as a knot and
builds a linear regression model using the candidate feature(s).

Consider non-linear, non-monotonic data where $Y = f(X)$. The MARS
method will initially search for a single point within a range of $X$
values where two distinct linear relationships between $Y$ and $X$
provide the lowest loss. The outcome is referred to as a hinge function
$h(x-a)$, where $a$ is the cutpoint value.

For example, if $a = 1$, our hinge function is $h(x-1)$ such that the
linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1 
\\ \beta_0 + \beta_1(x-1) \, ,& x > 1 \\ \end{cases}$$ After the first
knot is identified, the search for a second one begins, and it is
discovered at $x=2$. Now the linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1
\\ \beta_0 + \beta_1(x-1) \, ,& 1 < x < 2 
\\ \beta_0 + \beta_1(2-x) \, ,& x > 2 \\ \end{cases}$$ This process is
repeated until several knots are identified, leading to the creation of
a highly non-linear prediction equation. Even if using a lot of knots
could help us fit a particularly excellent relationship to our training
data, it might not perform well to unseen data. Once all of the knots
have been found, we may systematically eliminate knots that do not
significantly improve predictive accuracy. This is pruning process, and
we may use cross-validation to determine the optimal number of knots.

We will use the following packages. First of all, we divided the dataset
into training dataset and test dataset:

```{r}
library(dplyr)
library(ggplot2)
library(caret) 
library(earth)
library(vip)

set.seed(8888)

ind_mars <- which(runif(nrow(D_dtm)) < 0.7)

train_mars <- D_dtm %>% slice(ind_mars)
test_mars <- D_dtm %>% slice(-ind_mars)

cat("Dimensions of the training dataset are", dim(train_mars), "\n")
cat("Dimensions of the test dataset are", dim(test_mars), "\n")
```

MARS model have two hyperparameters: the maximum degree of interactions
and the number of terms retained in the final model. To achieve the
optimal combination of these tuning parameters, we must conduct a grid
search that minimize the error of prediction.

Here, we built up a grid with 30 different combinations of interaction
complexity (degree) and the number of terms to include in the final
model (nprune).

```{r}
marsgrid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 100, length.out = 20) %>% floor()
)
```

We performed required grid search by using 10-fold cross-validation:

```{r}
set.seed(9999)

mars_cv <- train( Y~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid
)

mars_cv$bestTune #nprune = 16, degree = 2

mars_cv$results %>%
  filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree)

ggplot(mars_cv)
```

The backwards elimination feature selection process used in MARS models
seeks for reductions in the generalized cross-validation (GCV) estimate
of error when each additional predictor is introduced to the model. The
variable importance is based on this overall reduction. MARS effectively
accomplishes automated feature selection since it will automatically
include and remove variables throughout the pruning phase.

After pruning, a predictor's significance value is 0 if it was never
used in any of the MARS basis functions in the final model. There are
only 17 features have importance values greater than 0, whereas the
other features all have importance values of zero since they were
excluded from the final model.

We also kept track of how the residual sums of squares (RSS) change when
terms are added. However, we noticed that there is no much difference
between these two measures.

```{r}
plot_GCV <- vip(mars_cv, num_features = 20, geom = "point", value = "gcv") + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "point", value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
```

We used the optimal hyperparameters to train the model and then
calculated the RMSE:

```{r}
set.seed(7777)

marsgrid_tuned <- expand.grid(
  degree = 2, 
  nprune = 16
)

mars_cv_tuned <- train( Y~., data = train_mars,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid_tuned
)

pred_mars = predict(mars_cv_tuned, test_mars)

print(caret::MAE(test_mars$Y, pred_mars)) #23.6061
print(caret::RMSE(test_mars$Y, pred_mars)) #34.19323
```

#### 3.1.1.5. Ridge Regression

```{r}
library(glmnet)

ind <- runif(nrow(D_dtm)) < 0.8
train_data = D_dtm[ind, ]
test_data = D_dtm[!ind, ]

#fit ridge regression model
model <- glmnet(x = train_data, y = train_data$Y, alpha = 0)
summary(model)
```

```{r}
#perform k-fold cross-validation to find optimal lambda value
lambda = 10^seq(from = 1, to = 4, length = 100)
ridge <- train(Y ~., data = train_data, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("scale")
)
ridge
```

```{r}
#find optimal lambda value that minimizes test MSE
best_lambda <- ridge$lambda.min
best_lambda
```

```{r}
#find coefficients of best model
best_model <- glmnet(x = train_data, y = train_data$Y, alpha = 1, lambda = best_lambda)
coef(best_model)
```

```{r}
#use fitted best model to make predictions
y_predicted <- predict(model, s = best_lambda, newx = train_data)
```

```{r}
#find SSE
sse <- sum((y_predicted - Y_test)^2)
n <- nrow(y_predicted)

#find RMSE
rmse <- sqrt((1/n) / sse)
rmse
```

#### 3.1.1.6. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a
modification of linear regression, as there is an extra regularization
term in its loss function as compared to that of linear regression. This
is to lower the complexity of the model by limiting the sum of the
absolute values of the coefficients. Meanwhile, elastic net regression
combines the properties of both linear and LASSO regressions, and there
are two hyperparameters, namely β and α. Here is the loss function of
elastic net regression: $$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$ We will need to choose the optimum values for these two
hyperparameters before generating predictions.

##### 3.1.1.6.1. Elastic Net

```{r}
elasticnet <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 5))

elasticnet
```

##### 3.1.1.6.2. LASSO

```{r}
lambda <- 10^seq(-3, 0, length = 20)
lambda
```

```{r}
lasso <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda), preProcess = c("scale"))

lasso
```

```{r}
lasso$bestTune
```

#### 3.1.1.7. Partial Least Squares (PLS)

Partial Least Squares (PLS) is a common technique to analyse relative
importance when the data includes more predictors than observations. It
is an useful dimension reduction method which is similar with principal
component analysis (PCA).

We do a regression against the response variable inside the narrower
space created by mapping the predictor variables to a smaller set of
variables. The response variable is not taken into account during the
dimension reduction process in PCA. PLS, on the other hand, seeks to
select newly mapped factors that best describe the response variable.

Below are the required packages. We divided the dataset into training
dataset and test dataset first:

```{r}
library(pls)

set.seed(77777)
# 80/20 split
inTraining <- createDataPartition(D_dtm$Y, p = .80, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]
```

The hyperparameter for PLS model is the number of components used in the
model (ncomp) .We conduct a grid search that minimize the prediction
error to achieve the optimal hyperparameter. The grid search was
conducted by 10-fold cross-validation:

```{r}
set.seed(88888)

plsGrid <- expand.grid(
  ncomp   = seq(1, 100, by = 1)
)

pls_cv <- train(
  Y ~ .,
  data = train_pls,
  method = 'pls',
  metric = "RMSE",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid
)

pls_cv

pls_cv$bestTune #6

plot(pls_cv)
```

We used the optimal hyperparameter to train the model and calculated the
RMSE as well:

```{r}
set.seed(99999)

plsGrid_tuned <- expand.grid(
  ncomp = 6
)

pls_cv_tuned <- train( Y~., data = train_pls,
  method = "pls",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid_tuned
)

pred_pls = predict(pls_cv_tuned, test_pls)

print(caret::MAE(test_pls$Y, pred_pls)) #19.02091
print(caret::RMSE(test_pls$Y, pred_pls)) #27.3268
```

The barplots below show that 'credit', 'lead', 'mission', 'actuarial'
and 'scientists' are positive predictors, while 'support', 'use',
'project', 'solutions' and 'food' are negative predictors:

```{r}
coefficients = coef(pls_cv_tuned$finalModel)
sum.coef = sum(sapply(coefficients, abs))
coefficients = coefficients * 100 / sum.coef
coefficients = sort(coefficients[, 1 , 1])

barplot(tail(coefficients, 5))
barplot(head(coefficients, 5))
```

### 3.1.2. Beyond the syllabus

#### 3.1.2.1. Shapley Value Regression

Shapley Value Regression is a technique used to determine the relative
importance of predictor variables in linear regression. It derives from
game theory and its purpose is to evaluate the worth of each player's
input over all possible combinations of players. In the study, it aims
to identify main variables predicting data scientists' salaries and to
avoid the problem of high correlation between variables.

```{r}
set.seed(1234)
ind <- runif(nrow(D_clean)) < 0.7

train_data <- D_clean[ind , ] 
test_data <- D_clean[!ind , ]

avg_S = train_data$Avg.Salary.K
rating = train_data$Rating
age = train_data$Age
comp = train_data$comp_count

lmsalary = lm(avg_S ~ comp + age  + rating)
summary(lmsalary)

library(relaimpo)
calc.relimp(lmsalary, type=list('lmg'), rela=T)

library(ggplot2)
plot(lm(avg_S ~ comp + age  + rating))

#relative importance metrics
names <- c("comp", "age", "rating")
count = c(0.2792880, 0.5205512, 0.2001608)

barplot(count,names.arg=names,xlab="Explanatory Variables",ylab="Relative Importance of Variables")
```

From the results, Age of the company is the most important predictor
variable. It accounts for approximately 52.1% of the average salary.
Established companies are more stable and have funds to provide higher
salaries as compared to start-ups. In contrast, other variables such as
number of competitors and rating only account for 27.9% and 20% of the
average salary. "comp" might be slightly higher than rating because
companies have to offer more attractive salaries to employees so they
will be less likely to job-hop to their competitor companies. \####
3.1.2.2. Theory behind the model

## 3.2. Summary of results

# 4. Conclusion

Recommendation engines are a commonly found solution applied to job
search portals, such as the Singapore government\'s national jobs
portal, MyCareersFuture. However, more can be done to bridge the gap
between job seekers and their desired careers. In this project, we have
produced models that predict the expected average salary earned given a
job description. Our chosen model, XGBoost, can be used to help workers
set expectations of salary based on keywords they value as important in
search of a job. This will help job seekers focus on searching for their
desired job role rather than focus on maximizing salary earned, which
will hopefully increase job satisfaction. The model also identifies key
terms such as \"machine\", \"phd\", \"education\", \"analyst\", and
\"infrastructure\", which are strongly related with data science
salaries, which job seekers can then use to help them focus on
upskilling in areas related to these key terms.

Based on RMSE, our best model uses XGBoost, and it has identified key
terms that have a significant impact on data science salary. However,
XGBoost does not identify the direction of relationship between the key
terms and predicted salary. Our linear regression models are thus useful
in this aspect, and further research could focus on extracting benefits
from both models.

\
\
