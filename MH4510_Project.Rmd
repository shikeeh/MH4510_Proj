---
title: "Data Science Salary Prediction"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# Team Humility

Chloe Neo Tze Ching Eng Jing Keat Goh Wei Lun Glenn Kaneko Yoshiki Tan
Wei Keong

## Abstract

Data mining is the process of sorting through large data sets to
identify patterns and relationships, where we aim to gain meaningful
insights through the use of algorithms to predict future trends in
textual data. In this project, we aim to uncover key insights in data
science salary by developing a model to effectively associate keywords
in job descriptions to high paying analytics roles. The data is cleaned
by filtering the keywords in job descriptions with a minimum occurrence
of 40. Least Absolute Shrinkage and Selection Operator (LASSO) is then
applied to select the most important variables to be used in the model.
The models used are Multiple Linear Regression (MLR), Partial Least
Squares (PLS), Random Forest, XGBoost and Multivariate Adaptive
Regression Splines (MARS), where their performances will be assess in
relation to one another. Across all models, XGBoost has the lowest Root
Mean Square Error (RMSE) of (insert value). In other words, XGBoost is
the best model that accurately predicts salary. Based on XGBoost
results, some of the most important keywords in job descriptions towards
getting a higher salary in a Data Scientist Role are \"machine\",
\"phd\", \"education\", \"analyst\", and \"infrastructure\".

# 1. Introduction to the problem

## 1.1. Literature review

A rapid shift towards digitalisation of businesses has radically changed
the employment landscape in Singapore, which means Singaporeans need to
keep up with the changes if they wish to stay competitive at work (My
Skills Future, 2021). Employers in Singapore are starting to place an
emphasis on skills rather than education (Tan, 2021). New hires today
are assessed not just by their qualifications and work history, but also
by their soft skills as there are a variety of soft skills in demand
(The Straits Times, 2021).

According to a new report by Instant Offices, 73% of Singaporean workers
are dissatisfied with their jobs. When asked if they planned to change
jobs over the following six months, 31% of respondents responded "yes".
Millions of workers worldwide are no longer willing to return home with
just a fair paycheck. They prefer to know how well they are progressing
towards a meaningful career, which is a wellness, freedom, security, and
experience at work, so as to achieve job satisfaction. As a result, job
seekers should take all these factors into consideration when applying
for a job.

These factors can be further broken down into keywords in job
descriptions. Keywords are crucial to job advertisements because they
allow job seekers to narrow their search related to a role, skill, or
industry for suitable employment. Ideal individuals are more likely to
find the job post when hirers and recruiters add key terms and phrases
that are relevant to a particular role. This increases the percentage of
successfully matching job seekers with their ideal jobs.

## 1.2. Objective

As most employers will be impressed when they notice that resume is
customised, highlighting relevant skills and using certain keywords
suited to that particular company or position, job seekers can use these
findings to narrow down their job search based on their own preferences
such as salary range or skills set. As such, this project aims to
predict data science salary based on the keywords in job descriptions.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  # create a new variable containing only lowercase text from Job.Description
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) 

head(D_clean)
```

### 2.2.2. Feature selection

Our project's focus is on job descriptions and their relationship with
data science salaries. As such, we will only keep these two variables

```{r}
#remove unnecessary columns 
D_clean <- D_clean %>%
  select(c(Avg.Salary.K., cleaned_text))
head(D_clean)
```

### 2.2.3. Data visualizations

1.  Histograms of average salaries in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram(fill = "blue", alpha = 0.65, bins = 20)
```

2.  Boxplots of average salaries:

```{r}
ggplot(D_clean, aes(y=Avg.Salary.K.)) +
  geom_boxplot(fill = "blue", alpha = 0.65)
```

3.  Word cloud for job descriptions:

```{r}
visualize_text <- function(x) {
  # x is a character vector
  # the function will extract
  frequent_words <- termFreq(x)
  frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
  wordcloud(words = names(frequent_words), 
            freq = frequent_words, min.freq = 40,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(9, "Dark2"))

}

visualize_text(D_clean$cleaned_text)
```

We set the minimum occurrence of words to 40, and show the 100 most
common words above.

### 2.2.4. Feature engineering

Next we create a document-term matrix for our job descriptions, setting
minimum word frequency to 40

```{r}
library(tm)
library(wordcloud)

corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
dtm <- DocumentTermMatrix(corpus)

words_freq <- termFreq(D_clean$cleaned_text)
frequent_words <- words_freq[words_freq >= 40]

cat("Setting minimum word frequency to 40, we retain", length(frequent_words), "words out of the original", length(words_freq), "words.", "\n")
```

```{r}
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
dtm <- dtm[ , names(frequent_words)]

cat("Our document-term matrix consists of", nrow(dtm), "job descriptions against", ncol(dtm), 'words present in our vocab after removing stopwords.', '\n')
```

Next, we create a new dataframe for our DTM

```{r}
D_dtm <- dtm %>%
  as.matrix %>%
  as_tibble %>% 
  mutate(Y = D_clean$Avg.Salary.K.)

head(D_dtm)
```

# 3. Objectives

## 3.1. Modelling

LASSO

We will try the following values of lambda for our LASSO regularization:

```{r}
lambda <- 10^seq(-1, 0 , length = 10)
lambda
```

Below we train our LASSO:

```{r}
set.seed(100)

lasso <- train(
  Y ~., data = D_dtm, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(alpha = 1, lambda = lambda),
  preProcess = c("center","scale")
)

lasso
```

Let us plot coefficients of LASSO

```{r}
lambda_for_plotting <- 10^seq(from = -1, to = 1.5, length = 100)
lasso_coefs <- coef(lasso$finalModel, lambda_for_plotting) %>%
  as.matrix %>% t %>% as_tibble %>%
  mutate(lambda = lambda_for_plotting)
head(lasso_coefs)

lasso_coefs %>%
  pivot_longer(-c(1, ncol(lasso_coefs)), names_to = "variable", values_to = "coef") %>%
  ggplot(aes(x = lambda, y = coef, group = variable, colour = variable)) +
  geom_line() + scale_x_log10() + theme(legend.position = "none")
```

Now we store our variables selected by LASSO

```{r}
store <- filter(lasso_coefs, lambda == lasso$bestTune$lambda) %>% 
  select_if(function(col) !all(col == 0))

selected_var <- names(store[2:(ncol(store)-1)])

head(selected_var)
```

Finally we prepare our final dataset to be used for our models

```{r}
D_final <- D_dtm[c(gsub("`","", selected_var))] %>%
  mutate(Y_salary = D_clean$Avg.Salary.K.) %>%
  select(c(Y_salary, everything()))

head(D_final)
```

Now we can prepare our training and test data:

```{r}
# Libraries used:
library(tidyverse)
library(stargazer)
library(caret)

set.seed(100)
ind <- runif(nrow(D_final)) < 0.8

train_data <- D_final[ind , ]
test_data <- D_final[!ind , ]

cat("Dimensions of the training set are", dim(train_data), "\n")
cat("Dimensions of the test set are", dim(test_data), "\n")
```

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

```{r}
mlr_mod <- lm(Y_salary ~ .,
               data = train_data)

mlr_mod %>%
    predict(test_data) %>%
    rmse(test_data$Y_salary)

data.frame(coefficients(mlr_mod)) %>% 
  arrange(desc(abs(coefficients(mlr_mod)))) %>%
  slice(2:21)
```

#### 3.1.1.2. Random forest

The next step is to visualize our data using decision trees. Nevertheless, 
decision trees alone have a significant variance, which can make the trees 
trained on various datasets appear quite distinct from one another. Random 
forest was chosen over a bagged ensemble as there may be one or more really 
powerful predictors in our dataset, preventing other predictors from having 
a chance to be included. As a result, the predictions made by the trees will 
be strongly correlated and increasing the number of trees will not make the 
variance smaller Therefore, in order to lower the variance, we shall propose 
an ensemble technique.

We performed grid search with oob (out-of-bag error) while tuning the
random forest to select the optimal values of the hyper-parameters try
(number of variables) and min.node.size (minimal node size):

```{r}
#getting optimal hyper parameters
mod_rf <- train(Y~., data = D_dtm, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))
mod_rf
```

The optimal values of the hyper-parameters are:

```{r}
mod_rf$bestTune
```

Now, we retrain the model with the selected hyperparameters to produce
the following output:

```{r}
rfGrid <- expand.grid(mtry = 1237,
                      min.node.size = 5,
                      splitrule = "variance")
                      
mod_rf_tune <- train(Y~., data = D_dtm, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
```

Next, we perform random forest on the train data set

```{r}
#random forest regression
train_data_rf <- train_data %>%
  select(-c(Y_salary))
test_data_rf <- test_data %>%
  select(-c(Y_salary))

regr <- randomForest(x = train_data_rf, y = train_data$Y_salary , maxnodes = 100 , ntree = 1000)
predictions <- predict(regr, test_data_rf)

result <- test_data
result['predictions'] <- predictions

head(result)
```

The graph below shows the data between Actual Average Salary vs.
Predicted Average Salary

```{r}
#plot graph to compare actual vs. predicted
ggplot(  ) + 
  geom_point( aes(x = test_data$Y_salary, y = predictions, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = test_data$Y_salary, y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Actual Average Salary", y = "Predicted Average Salary", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Actual", "Predicted"), values = c("navy", "green"))
```

```{r}
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , test_data$Y_salary) ))
print(paste0('RMSE: ' ,caret::postResample(predictions , test_data$Y_salary)['RMSE'])) #21.44
```

The bar graph below shows the top 20 most important variables in this
prediction. Based on the bar graph, the top 5 most important variables
are "machine", "give", "expression", "predictive", and "groups".

```{r}
#top 20 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(20)
var_importance
data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() + 
  xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45))
```

#### 3.1.1.3. XGBoost

Then we create our XGBoost model from caret, using hyperparameters as
shown below:

```{r}
library(caret)

set.seed(100)

trControl <- trainControl(
    method = 'cv',
    number = 5,
    verboseIter = TRUE,
    allowParallel = TRUE)

xgbGrid <- expand.grid(
  nrounds = 650,
  eta = 0.21,
  max_depth = 3,
  gamma = 0.04,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgbGrid_2 <- expand.grid(
  nrounds = seq(40, 100, length = 7),
  eta = seq(0.1, 0.6, length = 5),
  max_depth = seq(11, 20, length = 10),
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_caret <- train(Y_salary ~.,
                   data = train_data,
                   method = "xgbTree",
                   trControl = trControl,
                   tuneGrid = xgbGrid)

xgb_caret
```

```{r}
pred_y = predict(xgb_caret, test_data)

#measure prediction accuracy
caret::MAE(test_data$Y_salary, pred_y) #mae
caret::RMSE(test_data$Y_salary, pred_y) #rmse
```

```{r}
library(xgboost)
xgb_imp <- xgb.importance(feature_names = xgb_caret$finalModel$feature_names,
               model = xgb_caret$finalModel)

xgb_imp[1:20]

xgb.plot.importance(xgb_imp[1:20])
```

#### 3.1.1.4. Multivariate Adaptive Regression Splines (MARS)

We used multivariate adaptive regression splines (MARS) (Friedman 1991)
model here, it is an approach that automatically generates a piecewise
linear model that serves as an understandable stepping stone into
non-linearity after learning the notion of multiple linear regression.

By evaluating cutpoints (knots) similar to step functions, MARS offers a
practical method to capture the nonlinear relationships in the data.
This method evaluates every data point for every predictor as a knot and
builds a linear regression model using the candidate feature(s).

Consider non-linear, non-monotonic data where $Y = f(X)$. The MARS
method will initially search for a single point within a range of $X$
values where two distinct linear relationships between $Y$ and $X$
provide the lowest loss. The outcome is referred to as a hinge function
$h(x-a)$, where $a$ is the cutpoint value.

For example, if $a = 1$, our hinge function is $h(x-1)$ such that the
linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1 
\\ \beta_0 + \beta_1(x-1) \, ,& x > 1 \\ \end{cases}$$ After the first
knot is identified, the search for a second one begins, and it is
discovered at $x=2$. Now the linear models for $y$ are:

$$ y = \begin{cases} \beta_0 + \beta_1(1-x) \, ,& x < 1
\\ \beta_0 + \beta_1(x-1) \, ,& 1 < x < 2 
\\ \beta_0 + \beta_1(2-x) \, ,& x > 2 \\ \end{cases}$$ This process is
repeated until several knots are identified, leading to the creation of
a highly non-linear prediction equation. Even if using a lot of knots
could help us fit a particularly excellent relationship to our training
data, it might not perform well to unseen data. Once all of the knots
have been found, we may systematically eliminate knots that do not
significantly improve predictive accuracy. This is pruning process, and
we may use cross-validation to determine the optimal number of knots.

We will use the following packages. First of all, we divided the dataset
into training dataset and test dataset:

```{r}
library(dplyr)
library(ggplot2)
library(caret) 
library(earth)
library(vip)

set.seed(8888)

ind_mars <- which(runif(nrow(D_dtm)) < 0.7)

train_mars <- D_dtm %>% slice(ind_mars)
test_mars <- D_dtm %>% slice(-ind_mars)

cat("Dimensions of the training dataset are", dim(train_mars), "\n")
cat("Dimensions of the test dataset are", dim(test_mars), "\n")
```

MARS model have two hyperparameters: the maximum degree of interactions
and the number of terms retained in the final model. To achieve the
optimal combination of these tuning parameters, we must conduct a grid
search that minimize the error of prediction.

Here, we built up a grid with 30 different combinations of interaction
complexity (degree) and the number of terms to include in the final
model (nprune).

```{r}
marsgrid <- expand.grid(
  degree = 1:3, 
  nprune = seq(1, 100, length.out = 20) %>% floor()
)
```

We performed required grid search by using 10-fold cross-validation:

```{r}
set.seed(100)

mars_cv <- train(Y_salary ~., data = train_data,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid
)

mars_cv$bestTune #nprune = 21, degree = 3

mars_cv$results %>%
  filter(nprune == mars_cv$bestTune$nprune, degree == mars_cv$bestTune$degree)

ggplot(mars_cv)
```

The backwards elimination feature selection process used in MARS models
seeks for reductions in the generalized cross-validation (GCV) estimate
of error when each additional predictor is introduced to the model. The
variable importance is based on this overall reduction. MARS effectively
accomplishes automated feature selection since it will automatically
include and remove variables throughout the pruning phase.

After pruning, a predictor's significance value is 0 if it was never
used in any of the MARS basis functions in the final model. There are
only 17 features have importance values greater than 0, whereas the
other features all have importance values of zero since they were
excluded from the final model.

We also kept track of how the residual sums of squares (RSS) change when
terms are added. However, we noticed that there is no much difference
between these two measures.

```{r}
plot_GCV <- vip(mars_cv, num_features = 20, geom = "point", value = "gcv") + ggtitle("GCV")
plot_RSS <- vip(mars_cv, num_features = 20, geom = "point", value = "rss") + ggtitle("RSS")

gridExtra::grid.arrange(plot_GCV, plot_RSS, ncol = 2)
```

We used the optimal hyperparameters to train the model and then
calculated the RMSE:

```{r}
set.seed(100)

marsgrid_tuned <- expand.grid(
  degree = 3, 
  nprune = 21
)

mars_cv_tuned <- train(Y_salary ~., data = train_data,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = marsgrid_tuned
)

pred_mars = predict(mars_cv_tuned, test_data)

print(caret::MAE(test_data$Y_salary, pred_mars)) #23.6061
print(caret::RMSE(test_data$Y_salary, pred_mars)) #34.19323
```

#### 3.1.1.6. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a
modification of linear regression, as there is an extra regularization
term in its loss function as compared to that of linear regression. This
is to lower the complexity of the model by limiting the sum of the
absolute values of the coefficients. Meanwhile, elastic net regression
combines the properties of both linear and LASSO regressions, and there
are two hyperparameters, namely β and α. Here is the loss function of
elastic net regression: $$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$ We will need to choose the optimum values for these two
hyperparameters before generating predictions.

#### 3.1.1.7. Partial Least Squares (PLS)

Partial Least Squares (PLS) is a common technique to analyse relative
importance when the data includes more predictors than observations. It
is an useful dimension reduction method which is similar with principal
component analysis (PCA).

We do a regression against the response variable inside the narrower
space created by mapping the predictor variables to a smaller set of
variables. The response variable is not taken into account during the
dimension reduction process in PCA. PLS, on the other hand, seeks to
select newly mapped factors that best describe the response variable.

Below are the required packages. We divided the dataset into training
dataset and test dataset first:

```{r}
library(pls)

set.seed(100)
# 80/20 split
inTraining <- createDataPartition(D_dtm$Y, p = .80, list = FALSE)
train_pls <- D_dtm[inTraining,]
test_pls  <- D_dtm[-inTraining,]
```

The hyperparameter for PLS model is the number of components used in the
model (ncomp) .We conduct a grid search that minimize the prediction
error to achieve the optimal hyperparameter. The grid search was
conducted by 10-fold cross-validation:

```{r}
set.seed(88888)

plsGrid <- expand.grid(
  ncomp   = seq(1, 100, by = 1)
)

pls_cv <- train(
  Y ~ .,
  data = train_pls,
  method = 'pls',
  metric = "RMSE",
  preProcess = c("center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid
)

pls_cv

pls_cv$bestTune #6

plot(pls_cv)
```

We used the optimal hyperparameter to train the model and calculated the
RMSE as well:

```{r}
set.seed(99999)

plsGrid_tuned <- expand.grid(
  ncomp = 6
)

pls_cv_tuned <- train( Y~., data = train_pls,
  method = "pls",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = plsGrid_tuned
)

pred_pls = predict(pls_cv_tuned, test_pls)

print(caret::MAE(test_pls$Y, pred_pls)) #19.02091
print(caret::RMSE(test_pls$Y, pred_pls)) #27.3268
```

The barplots below show that 'credit', 'lead', 'mission', 'actuarial'
and 'scientists' are positive predictors, while 'support', 'use',
'project', 'solutions' and 'food' are negative predictors:

```{r}
coefficients = coef(pls_cv_tuned$finalModel)
sum.coef = sum(sapply(coefficients, abs))
coefficients = coefficients * 100 / sum.coef
coefficients = sort(coefficients[, 1 , 1])

barplot(tail(coefficients, 5))
barplot(head(coefficients, 5))
```

## 3.2. Summary of results

# 4. Conclusion

Recommendation engines are a commonly found solution applied to job
search portals, such as the Singapore government\'s national jobs
portal, MyCareersFuture. However, more can be done to bridge the gap
between job seekers and their desired careers. In this project, we have
produced models that predict the expected average salary earned given a
job description. Our chosen model, XGBoost, can be used to help workers
set expectations of salary based on keywords they value as important in
search of a job. This will help job seekers focus on searching for their
desired job role rather than focus on maximizing salary earned, which
will hopefully increase job satisfaction. The model also identifies key
terms such as \"machine\", \"phd\", \"education\", \"analyst\", and
\"infrastructure\", which are strongly related with data science
salaries, which job seekers can then use to help them focus on
upskilling in areas related to these key terms.

Based on RMSE, our best model uses XGBoost, and it has identified key
terms that have a significant impact on data science salary. However,
XGBoost does not identify the direction of relationship between the key
terms and predicted salary. Our linear regression models are thus useful
in this aspect, and further research could focus on extracting benefits
from both models.

\
\
