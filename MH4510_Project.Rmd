---
title: "Data Scientist Salary Prediction"
output:
  pdf_document: default
  html_document:
    df_print: paged
date: "`r Sys.Date()`"
editor_options: 
  markdown: 
    wrap: 72
---

# Team Humility

Chloe Neo Tze Ching
Eng Jing Keat
Goh Wei Lun Glenn
Kaneko Yoshiki
Tan Wei Keong

## Abstract

How to use Github:

1.  Open GitHub Desktop app
2.  Pull from origin to receive the most updated .Rmd file
3.  Make your edits on the MH4510_Project.Rmd file
4.  Save your edits in the same file
5.  Add descriptions for changes made
6.  Commit changes
7.  Push changes

Note that pulling the most update project file will override the project
file existing in your project folder. You may either create a fork to
save your changes, or simply create a separate .Rmd file to work on
personally before copying your edits to the main project file.

# 1. Introduction to the problem

## 1.1. Literature review

There is no doubt that COVID-19 has changed the employment landscape in
Singapore. A rapid shift towards digitalisation of businesses amidst a
recession means that Singaporeans have to keep pace with the change in
order to remain competitive at work (My Skills Future, 2021).

Singapore employers have prioritised skills over education (Tan, 2021).
Employers are starting to realise that new hires should be assessed
based on their existing skill sets --- instead of just their paper
qualifications and work history --- as there are various in-demand soft
skills transferable from one job to another. Other skills relevant to
the job may be learned through on-the-job training and development (The
Straits Times, 2021).

However, there is a gap between what individuals expect they will
require and what they have been taught in school among students. Just
27% of students say they are well equipped for future positions, while
22% say they are not at all prepared. According to a [2019
survey](https://www.cbi.org.uk/media/3841/12546_tess_2019.pdf) (Grimes,
2019), two out of five employers believe school and college graduates
are unprepared for employment. One-third say they are dissatisfied with
the quantity of relevant work experience young people have (EconoTimes,
2021).

The last decade has also shown us that whether or not one has a degree,
continual learning is imperative. In an age of ubiquitous disruption and
massive unpredictability, both employers and job seekers concede that
the knowledge and skills gained from a university degree can easily
become obsolete (Lim, 2021).

## 1.2. Objective

As the world grows more volatile, uncertain, complex and ambiguous,
graduates have to catch up with real-world needs and equip themselves
with the necessary skills. Considering that more employers are placing a
greater emphasis on individuals' skill sets and key traits than they are
on a piece of paper, this project aims to predict data scientist salary
based on the relevant variables in the dataset.

# 2. Dataset

Dataset:
<https://www.kaggle.com/datasets/nikhilbhathi/data-scientist-salary-us-glassdoor>

## 2.1. Description of dataset

Import relevant libraries:

```{r}
library(tidyverse)
library(tm)
library(wordcloud)
library(stringr)
library(GGally)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library(scales)
library(text2vec)
library(caret)
library(neuralnet)
library(fastDummies) # for creating dummy variables
library(randomForest)
library(Metrics)
library(keras)
library(tensorflow)

# import data from local csv file
D <- read.csv("data_cleaned_2021.csv")
head(D)
names(D)
```

This dataset has 41 variables, consisting of numerical, categorical, and
text. Some variables are derivations from others, and as such we will
not be using all 41 variables.

## 2.2. Exploratory data analysis

### 2.2.1. Data cleaning

Note: the dataset downloaded has already been cleaned by the owner, but
we will do some additional cleaning and data preparation so that it is
suited for our needs.

1.  Removing "\\n" from job descriptions, cleaning job descriptions
    text, and creating a new variable to store lengths of job
    description texts:

```{r}
# Keep only alphabets and spaces, changing texts to lowercase, and create a new variable to store length of job description texts
D_clean <- D %>%
  mutate(cleaned_text = gsub("[^a-zA-Z0-9]", " ", Job.Description)) %>%
  mutate(cleaned_text = gsub("\\n", " ", cleaned_text)) %>%
  mutate(cleaned_text = tolower(cleaned_text)) %>%
  mutate(Job.Title = tolower(Job.Title)) %>%
  mutate('desc_len' = sapply(D$Job.Description, nchar))

# Ssing stringr to replace all instances of "\n" with spaces instead
#while (grepl("\n", D_clean$cleaned_text, fixed = TRUE)) {
#  D_clean$cleaned_text <- str_replace(D_clean$cleaned_text, "\n", " ")
#}
# Used a while loop because the function would only run once through the string, removing only one instance of '\n'

head(D_clean)
```

2.  Re-doing columns to reflect simplified job titles and seniority:

```{r}
# Drop the versions that the owner created, so we can control how it is done
D_clean <- D_clean %>%
  select(-c('job_title_sim', 'seniority_by_title'))

# Create a function to collate general titles of job positions
job_simp_fn <- function(title) {
  if (grepl("data scientist", title)) {
  output <- "data scientist"
  } else if (grepl("director", title)) {
    output <- "director"
  } else if (grepl("manager", title)) {
    output <- "manager"
  } else if (grepl("data science", title)) {
    output <- "data scientist"
  } else if (grepl("data engineer", title)) {
    output <- "data engineer"
  } else if (grepl("data analyst", title)) {
    output <- "data analyst"
  } else if (grepl("data analytics", title)) {
    output <- "data analyst"
  } else if (grepl("machine learning", title)) {
    output <- "MLE"
  } else {
    output <- "NA"
  }}

# Create a function to collate seniority of job positions
seniority <- function(title) {
  if (grepl("sr", title)) {
  output <- "senior"
  } else if (grepl("senior", title)) {
    output <- "senior"
  } else if (grepl("jr", title)) {
    output <- "junior"
  } else if (grepl("junior", title)) {
    output <- "junior"
  } else {
    output <- 'NA'
  }}

# Apply both functions and create new variables 
D_clean$'job_simp' <- sapply(D_clean$Job.Title, job_simp_fn)
D_clean$'seniority' <- sapply(D_clean$Job.Title, seniority)

# View head of only these three variables:
head(D_clean[, c('Job.Title', 'job_simp', 'seniority')])
```

3.  Create a new variable to count the number of competitors a company
    has:

```{r}
# Function to convert "Competitors" variable from "-1" and strings to a count variable
my_strsplit <- function(string) {
  if (string == -1){
    output <- 0
  } else {
  output <- length(strsplit(toString(string), ", ")[[1]])
  }}

# Apply the function and create a new variable comp_count
D_clean$comp_count <- sapply(D_clean$Competitors, my_strsplit)

# View head of variables 'Competitors' and 'comp_count'
head(D_clean[, c('Competitors', 'comp_count')])
```

4. Variables Selection

After removing unnecessary columns, our data contains 33 variables for analysis.

```{r}
#remove unnecessary columns 
D_clean <- D_clean %>%
select(-c(index, Job.Title, Salary.Estimate, Job.Description, Company.Name, Location, Founded, Competitors, Lower.Salary, Upper.Salary, job_simp))
names(D_clean)
```

### 2.2.2. Data visualizations

Below is a summary of our numeric variables:

(this included categorical variables that used 1s and 0s, such as
python, employer.provided, etc. and we have yet to remove them).

```{r}
summary(D_clean%>%select_if(is.numeric))
```

#### 2.2.2.1. Numerical variables data visualizations

1.  Histograms

Histogram of rating:

```{r}
ggplot(D_clean, aes(x=Rating)) +
  geom_histogram()
```

-   Companies without rating are given a -1 rating. Since the rest of
    the ratings follow a normal distribution, we may replace the ratings
    with a mean value instead.

Histogram of average salary in thousands:

```{r}
ggplot(D_clean, aes(x=Avg.Salary.K.)) + 
  geom_histogram()
```

Histogram of age of companies:

```{r}
ggplot(D_clean, aes(x=Age)) + 
  geom_histogram()
```

Histogram of job description text length:

```{r}
ggplot(D_clean, aes(x=desc_len)) +
  geom_histogram()
```

2.  Boxplots

Boxplot of salaries

```{r}
D_long <- melt(D_clean%>%select(c(Lower.Salary, Upper.Salary, Avg.Salary.K.)))
head(D_long)
ggplot(D_long, aes(x=variable, y=value)) +
  geom_boxplot()
```

Boxplot of Rating:

```{r}
ggplot(D_clean, aes(y=Rating)) +
  geom_boxplot()
```

Boxplot of Age:

```{r}
ggplot(D_clean, aes(y=Age)) +
  geom_boxplot()
```

Boxplot of Job Description Length:

```{r}
ggplot(D_clean, aes(y=desc_len)) + 
  geom_boxplot()
```

3.  Word cloud

a\. Generating our term-document matrix:

```{r}
corpus <- VCorpus(VectorSource(D_clean$cleaned_text))
DTM <- DocumentTermMatrix(corpus)
dim(DTM)
```

-   (perhaps we can try GloVe instead)

Some frequent words, organized according to alphabetical order:

```{r}
words_freq <- termFreq(D_clean$cleaned_text)
head(words_freq)
```

b\. Some of these words do not make sense, so we increase the minimum
frequency of the word to 35 (We noticed at words_freq \>= 30 there were
still some hard to understand words, like abl = 33):

```{r}
frequent_words <- words_freq[words_freq >= 35]
length(frequent_words)
```

c\. The new dimensions of our term-document matrix with the frequent
words:

```{r}
DTM <- DTM[ , names(frequent_words)]
dim(DTM)
```

d\. Remove stopwords and plot word cloud. (Plus add some colours):

```{r}
frequent_words <- frequent_words[!(names(frequent_words) %in% stopwords())]
wordcloud(words = names(frequent_words), freq = frequent_words, min.freq = 0,
            max.words = 100, random.order=FALSE, rot.per=0.35, 
            colors=brewer.pal(8, "Dark2"))
```

e\. Dimensions of term-document matrix after removing stopwords:

```{r}
DTM <- DTM[ , names(frequent_words)]
dim(DTM)
```

4.  Corrplot

```{r}
D_num <- D_clean %>%
  select(-c("index", "Python", "spark", "aws", "excel", "sql", "sas", "keras", "pytorch",  
            "scikit", "tensor", "hadoop", 'tableau', 'bi', 'flink', 'mongo', 'google_an', 
            "Employer.provided", 'Hourly')) %>% 
  rename('comp' = 'comp_count', 'desc' = 'desc_len', 'avg_S' = 'Avg.Salary.K.', 
         'upp_S' = 'Upper.Salary', 'low_S' = 'Lower.Salary')

D_num %>%
  ggcorr(palette = "RdBu", label = TRUE)
```

```{r}
D_num %>% 
  select_if(is.numeric) %>%
  ggpairs()
```

```{r}
D_num %>%
  select_if(is.numeric) %>%
  cor %>% round(3)
```

#### 2.2.2.2. Categorical variables data visualizations

Calling out names of variables in our dataset to find which are
categorical:

```{r}
names(D_clean)
```

Creating a new dataframe consisting of categorical variables:

```{r}
D_cat <- D_clean %>%
  select(c("Location", "Headquarters", "Size", "Type.of.ownership", "Industry", 
           "Sector", "Revenue", "company_txt", "Job.Location", 'Python', "spark", "aws", 
           "excel", "sql", "sas", "keras", "pytorch", "scikit", "tensor", "hadoop", 
           "tableau", "bi", "flink", "mongo", "google_an", "job_simp", "seniority"))
head(D_cat)
```

-   Note that variables like "Location", "Headquarters", "Industry", and
    "company_txt" have too many variables to properly plot the graphs

1.  Barplot of top 10 job locations (state):

```{r}
D_cat %>%
  count(Job.Location) %>%
  mutate(n_percent = prop.table(n)) %>%
  arrange(-n_percent) %>%
  slice(1:10) %>%
  ggplot(aes(x = reorder(Job.Location, -n_percent), y = n, fill = n, 
             label = scales::percent(n_percent))) + 
  geom_bar(position='dodge', stat="identity") + 
  geom_text(size=3, vjust = -0.5) + 
  geom_text(size=3, aes(label = n), vjust = 1.5, colour="white") +
  xlab("Job Location (State)") + ylab("N")
```

```{r}
ggplot(D_cat, aes(x=job_simp))
```

## 2.3. Feature engineering

### 2.3.1. Identifying variables

### 2.3.2. Variable selection

### 2.3.3. GloVe Word Embedding

In this report, we used an unsupervised word embedding technique, which is GloVe to achieve vector representations for words inside the cleaned_text.

First of all, we create the term co-occurrence matrix (TCM):
```{r}
tokens <- space_tokenizer(D_clean$cleaned_text)
it = itoken(tokens, progressbar = FALSE)
vocab <- create_vocabulary(it) %>% prune_vocabulary(10)
vectorizer <- vocab_vectorizer(vocab)
tcm <- create_tcm(it, vectorizer)

cat("Dimensions of tcm =", dim(tcm), "\n")
```

Then, we create the Global Vectors (GloVe):
```{r}
dim_word_emb <- 100

glove = GlobalVectors$new(rank = dim_word_emb, x_max = 10)

wv_main = glove$fit_transform(tcm, n_iter = 50, convergence_tol = 0.01)

wv_context = glove$components
word_vectors = wv_main + t(wv_context)

cat("Dim of word vector matrix =", dim(word_vectors))
```


# 3. Objectives

## 3.1. Modelling

### 3.1.1. Proposed models

#### 3.1.1.1. Multiple linear regression

#### 3.1.1.2. Random forest/deep forest (gcforest)/XGBoost

#Random Forest

Next, we want to visualise our data through the use of decision trees. However, decision trees alone have a high variance which can cause the trees trained on different parts of the dataset to look very different. Hence, we will introduce an ensemble method to reduce the variance. Random forest is chosen over a bagged ensemble as there may exist one or more very strong predictors in our dataset, causing other predictors to not have a chance to be included. This will cause the predictions of trees to be highly correlated and adding more trees will not reduce the variance. Variable selection can also be done by looking at the variable importance of the random forest.

We performed grid search with oob (out-of-bag error) while tuning the random forest to select the optimal values of the hyper-parameters try (number of variables) and min.node.size (minimal node size):

```{r}
#getting optimal hyper parameters
mod_rf <- train(Avg.Salary.K.~., data = D_clean_1, method = "ranger",
                num.trees = 50, importance = 'impurity',
                trControl = trainControl("oob"))
mod_rf
```
The optimal values of the hyper-parameters are:

```{r}
mod_rf$bestTune
```

Now, we retrain the model with the selected hyperparameters to produce the following output:

```{r}
rfGrid <- expand.grid(mtry = 1379,
                      min.node.size = 5,
                      splitrule = "variance")
                      
mod_rf_tune <- train(Avg.Salary.K.~., data = D_clean_1, method = "ranger",
                     num.trees = 50, importance = 'impurity',
                     tuneGrid = rfGrid, trControl = trainControl("oob"))
mod_rf_tune
```

Then, we split our data into train and test data sets:

```{r}
#splitting into train and test data
X <- D_clean_1 %>% 
  select(-Avg.Salary.K.)
y <- D_clean_1$Avg.Salary.K.

index <- createDataPartition(y, p=0.7, list=FALSE)
X_train <- X[index, ]
X_test <- X[-index, ]
Y_train <- y[index]
Y_test<- y[-index]

cat("Dimensions of the training set are", dim(X_train), "\n")
cat("Dimensions of the test set are", dim(X_test), "\n")
```

Next, we perform random forest on the train data set

```{r}
#random forest regression
regr <- randomForest(x = X_train, y = Y_train , maxnodes = 100, ntree = 1000)
predictions <- predict(regr, X_test)

result <- X_test
result['Avg.Salary.K.'] <- Y_test
result['prediction']<-  predictions

head(result)
```
The graph below shows the data between Actual Average Salary vs. Predicted Average Salary

```{r}
#plot graph to compare actual vs. predicted
ggplot(  ) + 
  geom_point( aes(x = X_test$Rating, y = predictions, color = 'red', alpha = 0.5) ) + 
  geom_point( aes(x = X_test$Rating , y = predictions, color = 'blue',  alpha = 0.5)) + 
  labs(x = "Rating", y = "Avg.Salary.K", color = "", alpha = 'Transperency') +
  scale_color_manual(labels = c( "Predicted", "Real"), values = c("blue", "red"))
```

```{r}
#using metrics to calculate RMSE
print(paste0('MAE: ' , mae(predictions , Y_test) ))
print(paste0('MSE: ' ,caret::postResample(predictions , Y_test)['RMSE']^2 ))
print(paste0('RMSE: ' ,caret::postResample(predictions , Y_test)['RMSE'])) #27.12
print(paste0('R2: ' ,caret::postResample(predictions , Y_test)['Rsquared'] ))
```
The bar graph below shows the top 5 most important variables in this prediction

```{r}
#top 5 most important predictors 
var_importance = mod_rf_tune$finalModel$variable.importance %>% 
  sort(decreasing = TRUE) %>% head(5)
data.frame(variable = names(var_importance), importance = var_importance) %>% 
  ggplot(aes(x = reorder(variable, -importance), y = importance)) + geom_col() + 
  xlab("Variables") + ylab("Importance") + theme(axis.text.x = element_text(angle = 45)) 
```

#### 3.1.1.3. Neural network

In order to predict the salary with neural network, we used Continuous Bag of Words (CBOW) as follows. It enables us to do downstream tasks, such as regression on texts.
```{r}
dtm <- create_dtm(it, vectorizer)
cbow_data <- as.matrix(dtm %*% word_vectors)
```

Here is the model:
```{r}
set.seed(8888)
all_data <- cbow_data %>%
  as_tibble %>%
  mutate(Y = D_clean$Avg.Salary.K.)

ind <- which(runif(nrow(all_data)) < 0.7)

train_data <- all_data %>% slice(ind)
test_data <- all_data %>% slice(-ind)

mod_nn <- neuralnet(Y ~ ., data = train_data, 
                                    hidden = c(20, 20),
                    threshold = 0.5,
                    lifesign = "full",
                    lifesign.step = 100,
                    stepmax = 10000,
                    err.fct = "sse",
                    linear.output = TRUE)

pred_nn <- compute(mod_nn,test_data[,1:100])
RMSE_nn <- (sum((test_data$Y - pred_nn$net.result)^2)/nrow(test_data))^0.5

RMSE_nn #41.20078
```

#### 3.1.1.4. Symbolic regression

#### 3.1.1.5. Elastic net regression/LASSO

The Least Absolute Shrinkage and Selection Operator (LASSO) is a modification
of linear regression, as there is an extra regularization term in its loss
function as compared to that of linear regression. This is to lower the 
complexity of the model by limiting the sum of the absolute values of the 
coefficients. Meanwhile, elastic net regression combines the properties of 
both linear and LASSO regressions, and there are two hyperparameters, namely 
β and α. Here is the loss function of elastic net regression:
$$
L_E(\beta)=\sum_{i=1}^{N}
\left(y^i - \beta_0 - \sum_{j=1}^{p}\beta_jx_j^i\right)^2+
(1-\alpha)\lambda\sum_{j=1}^{p}\beta_j^2+
\alpha\lambda\sum_{j=1}^{p}|\beta_j|
$$
We will need to choose the optimum values for these two hyperparameters 
before generating predictions.

##Elastic Net
```{r}
elasticnet <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 5))

elasticnet
```

##LASSO
```{r}
lambda <- 10^seq(-3, 0, length = 20)
lambda
```

```{r}
lasso <- train(Avg.Salary.K. ~., data = D_clean, method = "glmnet", trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 1, lambda = lambda), preProcess = c("scale"))

lasso
```
```{r}
lasso$bestTune
```

### 3.1.2. Beyond the syllabus

#### 3.1.2.1. Shapley Value Regression
Shapley Value Regression is a technique used to determine the relative importance of predictor variables in linear regression. It derives from game theory and its purpose is to evaluate the worth of each player's input over all possible combinations of players. In the study, it aims to identify main variables predicting data scientists' salaries and to avoid the problem of high correlation between variables.

```{r}
set.seed(1234)
ind <- runif(nrow(D_clean)) < 0.7

train_data <- D_clean[ind , ] 
test_data <- D_clean[!ind , ]

avg_S = train_data$Avg.Salary.K
rating = train_data$Rating
age = train_data$Age
comp = train_data$comp_count

lmsalary = lm(avg_S ~ comp + age  + rating)
summary(lmsalary)

library(relaimpo)
calc.relimp(lmsalary, type=list('lmg'), rela=T)

library(ggplot2)
plot(lm(avg_S ~ comp + age  + rating))

#relative importance metrics
names <- c("comp", "age", "rating")
count = c(0.2792880, 0.5205512, 0.2001608)

barplot(count,names.arg=names,xlab="Explanatory Variables",ylab="Relative Importance of Variables")
```

From the results, Age of the company is the most important predictor variable. It accounts for approximately 52.1% of the average salary. Established companies are more stable and have funds to provide higher salaries as compared to start-ups. In contrast, other variables such as number of competitors and rating only account for 27.9% and 20% of the average salary. "comp" might be slightly higher than rating because companies have to offer more attractive salaries to employees so they will be less likely to job-hop to their competitor companies.
#### 3.1.2.2. Theory behind the model

## 3.2. Summary of results

# 4. Conclusion
